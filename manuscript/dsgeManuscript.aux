\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{SmetsWouters2007}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{tEoSL-2nd}
\citation{Stone1974,Geisser-predictive-sample-reuse,Geisser-Eddy-predictive-approach}
\citation{Edge-Gurkaynak-on-dsges}
\citation{Spirtes-Glymour-Scheines-1st,Pearl-causality}
\citation{Morgan-Winship-counterfactuals-2nd}
\citation{Lucas1976}
\citation{KydlandPrescott1982}
\citation{DeJongDave2007}
\citation{KydlandPrescott1982}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Brief Remarks on the Literature}{2}{subsection.1.1}\protected@file@percent }
\newlabel{brief-remarks-on-the-literature}{{1.1}{2}{Brief Remarks on the Literature}{subsection.1.1}{}}
\citation{SmetsWouters2007}
\citation{Kirman-contra-representative-agent}
\citation{Jackson-Yariv-non-existence-of-representative-agents}
\citation{Edge-Gurkaynak-on-dsges}
\citation{Komunjer-Ng-identification-of-DSGEs}
\citation{Iskrev2009}
\citation{SmetsWouters2007}
\citation{Sims2002}
\citation{DeJongDave2007}
\citation{BlanchardKahn1980,Klein2000}
\citation{Sims2002}
\citation{Kalman1960}
\@writefile{toc}{\contentsline {section}{\numberline {2}Specification and Baseline Estimation of the DSGE}{4}{section.2}\protected@file@percent }
\newlabel{specification-and-baseline-estimation-of-the-dsge}{{2}{4}{Specification and Baseline Estimation of the DSGE}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Solving DSGEs}{4}{subsection.2.1}\protected@file@percent }
\newlabel{sec:solveDSGEs}{{2.1}{4}{Solving DSGEs}{subsection.2.1}{}}
\newlabel{eq:lin-dsge}{{3}{4}{Solving DSGEs}{equation.2.3}{}}
\newlabel{eq:ss-system}{{4}{4}{Solving DSGEs}{equation.2.4}{}}
\newlabel{eq:obs-equation}{{5}{4}{Solving DSGEs}{equation.2.5}{}}
\citation{Sims2002}
\citation{SmetsWouters2007}
\citation{SmetsWouters2007}
\citation{SmetsWouters2007}
\citation{Iskrev2009}
\citation{SmetsWouters2007}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Pseudoalgorithm for estimating linear rational expectations models}}{5}{algorithm.1}\protected@file@percent }
\newlabel{alg:solveRE}{{1}{5}{Solving DSGEs}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Estimating the \citet  {SmetsWouters2007} model}{5}{subsection.2.2}\protected@file@percent }
\newlabel{sec:estim-sw-model}{{2.2}{5}{Estimating the \citet {SmetsWouters2007} model}{subsection.2.2}{}}
\citation{SmetsWouters2007}
\citation{SmetsWouters2007}
\citation{SmetsWouters2007}
\citation{SmetsWouters2007}
\citation{SmetsWouters2007}
\@writefile{toc}{\contentsline {section}{\numberline {3}Simulate and estimate}{6}{section.3}\protected@file@percent }
\newlabel{sec:simulate-estimate}{{3}{6}{Simulate and estimate}{section.3}{}}
\newlabel{sec:simulate-estimate}{{3}{6}{Simulate and estimate}{section.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Prior distributions, posterior modes, and posterior mode as estimated by \citep  {SmetsWouters2007} for the 36 estimated parameters. All values are rounded to two decimal places.}}{7}{table.1}\protected@file@percent }
\newlabel{tab:parameter-estimate-table}{{1}{7}{Prior distributions, posterior modes, and posterior mode as estimated by \citep {SmetsWouters2007} for the 36 estimated parameters. All values are rounded to two decimal places}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Training (in sample) error averaged across series as the number of training points increases. We would expect the variability to decline while the average remains constant. The mean is shown in red along with 30\%, 60\%, 80\%, and 90\% confidence bands calculated from the replications.}}{8}{figure.1}\protected@file@percent }
\newlabel{fig:train-error}{{1}{8}{Training (in sample) error averaged across series as the number of training points increases. We would expect the variability to decline while the average remains constant. The mean is shown in red along with 30\%, 60\%, 80\%, and 90\% confidence bands calculated from the replications}{figure.1}{}}
\citation{Andy-Fraser-on-HMMs}
\citation{Gray-entropy-2nd}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Out-of-sample error averaged across series as the number of training points increases. We would expect both the variability and the average to decline. The blue line is the test error for the true model.}}{9}{figure.2}\protected@file@percent }
\newlabel{fig:test-error}{{2}{9}{Out-of-sample error averaged across series as the number of training points increases. We would expect both the variability and the average to decline. The blue line is the test error for the true model}{figure.2}{}}
\citation{Algoet-and-Cover-on-AEP}
\citation{Gray-entropy-2nd}
\citation{SmetsWouters2007}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The parameter MSE shows similar behavior to the predictive MSE: steep initial decline toward an asymptote greater than that of the true model.}}{10}{figure.3}\protected@file@percent }
\newlabel{fig:parm-error}{{3}{10}{The parameter MSE shows similar behavior to the predictive MSE: steep initial decline toward an asymptote greater than that of the true model}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The negative log-likelihood (in-sample) per observation.}}{11}{figure.4}\protected@file@percent }
\newlabel{fig:pen-negll}{{4}{11}{The negative log-likelihood (in-sample) per observation}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Permuting the data}{11}{section.4}\protected@file@percent }
\newlabel{sec:permutation-summary}{{4}{11}{Permuting the data}{section.4}{}}
\newlabel{sec:permutation-summary}{{4}{11}{Permuting the data}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The negative predictive log-likelihood (out-of-sample) per observation.}}{12}{figure.5}\protected@file@percent }
\newlabel{fig:predict-negll}{{5}{12}{The negative predictive log-likelihood (out-of-sample) per observation}{figure.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Series MSEs for SW and top models.}}{13}{table.2}\protected@file@percent }
\newlabel{tab:best-results}{{2}{13}{Series MSEs for SW and top models}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Permutations with highest \% improvement}}{13}{table.3}\protected@file@percent }
\newlabel{tab:top-20-perc}{{3}{13}{Permutations with highest \% improvement}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Permutations with lowest average scaled MSE}}{14}{table.4}\protected@file@percent }
\newlabel{tab:top-20-scmse}{{4}{14}{Permutations with lowest average scaled MSE}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Permutations with lowest negative log likelihood}}{14}{table.5}\protected@file@percent }
\newlabel{tab:top-20-llike}{{5}{14}{Permutations with lowest negative log likelihood}{table.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Mean-squared prediction error for each series.}}{15}{figure.6}\protected@file@percent }
\newlabel{fig:individual-series}{{6}{15}{Mean-squared prediction error for each series}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Out-of-sample forecasts}{15}{subsection.4.1}\protected@file@percent }
\newlabel{out-of-sample-forecasts}{{4.1}{15}{Out-of-sample forecasts}{subsection.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Estimates for the standard deviations of stochastic shocks.}}{16}{figure.7}\protected@file@percent }
\newlabel{fig:shock-sds}{{7}{16}{Estimates for the standard deviations of stochastic shocks}{figure.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Things Waldman asked which haven't appeared yet}{16}{section.5}\protected@file@percent }
\newlabel{things-waldman-asked-which-havent-appeared-yet}{{5}{16}{Things Waldman asked which haven't appeared yet}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Do the deep parameters covary with policy parameters?}{16}{subsection.5.1}\protected@file@percent }
\newlabel{do-the-deep-parameters-covary-with-policy-parameters}{{5.1}{16}{Do the deep parameters covary with policy parameters?}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Changing the truth}{16}{subsection.5.2}\protected@file@percent }
\newlabel{changing-the-truth}{{5.2}{16}{Changing the truth}{subsection.5.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Correlations between 'deep' parameters and Taylor rule parameters. From the 'Simulate and estimate' exercise.}}{17}{table.6}\protected@file@percent }
\newlabel{tab:covary-with-Taylor}{{6}{17}{Correlations between 'deep' parameters and Taylor rule parameters. From the 'Simulate and estimate' exercise}{table.6}{}}
\citation{SmetsWouters2007}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Estimates for the autocorrelation parameters in the shock processes.}}{18}{figure.8}\protected@file@percent }
\newlabel{fig:autocorrelations-shocks}{{8}{18}{Estimates for the autocorrelation parameters in the shock processes}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Best model by predictive log-likelihood}{18}{subsection.5.3}\protected@file@percent }
\newlabel{best-model-by-predictive-log-likelihood}{{5.3}{18}{Best model by predictive log-likelihood}{subsection.5.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{18}{section.6}\protected@file@percent }
\newlabel{conclusions}{{6}{18}{Conclusions}{section.6}{}}
\citation{Haavelmo-probability-approach}
\citation{Boivin-Giannoni-DSGEs-in-data-rich-environment}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Estimates of "deep" parameters}}{19}{figure.9}\protected@file@percent }
\newlabel{fig:deep1}{{9}{19}{Estimates of "deep" parameters}{figure.9}{}}
\citation{KydlandPrescott1982}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Estimates of "deep" parameters}}{20}{figure.10}\protected@file@percent }
\newlabel{fig:deep2}{{10}{20}{Estimates of "deep" parameters}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Estimates of "deep" parameters}}{21}{figure.11}\protected@file@percent }
\newlabel{fig:deep3}{{11}{21}{Estimates of "deep" parameters}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Comparing the in-sample negative loglikelihood per observation of a parameter estimated with 1000 observations relative to the true parameter. In the long-run, the entropy rate is a lower bound, but only after about 12,000 years of quarterly data.}}{21}{figure.12}\protected@file@percent }
\newlabel{fig:entropy-investigation}{{12}{21}{Comparing the in-sample negative loglikelihood per observation of a parameter estimated with 1000 observations relative to the true parameter. In the long-run, the entropy rate is a lower bound, but only after about 12,000 years of quarterly data}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Average percentage improvement in out-of-sample forecast MSE. The horizontal red line represents baseline performance. Lower values are better.}}{22}{figure.13}\protected@file@percent }
\newlabel{fig:mean-percent-improvement}{{13}{22}{Average percentage improvement in out-of-sample forecast MSE. The horizontal red line represents baseline performance. Lower values are better}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Percentage improvement in out-of-sample forecast MSE for each data series individually. The horizontal red line represents baseline performance. Values to the left are better.}}{22}{figure.14}\protected@file@percent }
\newlabel{fig:series-percent-improvement}{{14}{22}{Percentage improvement in out-of-sample forecast MSE for each data series individually. The horizontal red line represents baseline performance. Values to the left are better}{figure.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Average improvement in out-of-sample forecast MSE scaled by the variance of the data. The horizontal red line represents baseline performance.}}{23}{figure.15}\protected@file@percent }
\newlabel{fig:mean-scaled-mse}{{15}{23}{Average improvement in out-of-sample forecast MSE scaled by the variance of the data. The horizontal red line represents baseline performance}{figure.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Improvement in out-of-sample forecast MSE scaled by the variance of the data for each data series individually. The red dots represents baseline performance.}}{23}{figure.16}\protected@file@percent }
\newlabel{fig:series-scaled-mse}{{16}{23}{Improvement in out-of-sample forecast MSE scaled by the variance of the data for each data series individually. The red dots represents baseline performance}{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Negative log predictive likelihood for each model. The horizontal red line represents baseline performance. Note that this is an out-of-sample performance measure.}}{24}{figure.17}\protected@file@percent }
\newlabel{fig:loglike-pred-ml}{{17}{24}{Negative log predictive likelihood for each model. The horizontal red line represents baseline performance. Note that this is an out-of-sample performance measure}{figure.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Parameter estimates rescaled to [0,1] based on the prior limits. Red points indicate the SW estimates. Black points are outliers relative to the bulk of the permuted estimates.}}{25}{figure.18}\protected@file@percent }
\newlabel{fig:scaled-parameter-boxplots}{{18}{25}{Parameter estimates rescaled to [0,1] based on the prior limits. Red points indicate the SW estimates. Black points are outliers relative to the bulk of the permuted estimates}{figure.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Percentage change in estimated parameter relative to the SW estimates. Some large outliers have been removed to better show the bulk.}}{26}{figure.19}\protected@file@percent }
\newlabel{fig:perc-parameter-deviation}{{19}{26}{Percentage change in estimated parameter relative to the SW estimates. Some large outliers have been removed to better show the bulk}{figure.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Out-of-sample predictions for the best predicting permutations (black), the SW model (red), and the observed data (blue).}}{27}{figure.20}\protected@file@percent }
\newlabel{fig:pc-preds-p1}{{20}{27}{Out-of-sample predictions for the best predicting permutations (black), the SW model (red), and the observed data (blue)}{figure.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Out-of-sample predictions for the best predicting permutations (black), the SW model (red), and the observed data (blue).}}{28}{figure.21}\protected@file@percent }
\newlabel{fig:pc-preds-p2}{{21}{28}{Out-of-sample predictions for the best predicting permutations (black), the SW model (red), and the observed data (blue)}{figure.21}{}}
\citation{SmetsWouters2007}
\@writefile{toc}{\contentsline {section}{\numberline {A}Data preprocessing}{29}{appendix.A}\protected@file@percent }
\newlabel{sec:data-preprocessing}{{A}{29}{Data preprocessing}{appendix.A}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Data series from FRED for estimating the DSGE.}}{29}{table.7}\protected@file@percent }
\newlabel{tab:data}{{7}{29}{Data series from FRED for estimating the DSGE}{table.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Rewriting \citet  {SmetsWouters2007}}{29}{appendix.B}\protected@file@percent }
\newlabel{rewriting}{{B}{29}{\texorpdfstring {Rewriting \citet {SmetsWouters2007}}{Rewriting }}{appendix.B}{}}
\newlabel{eq:1}{{9}{29}{\texorpdfstring {Rewriting \citet {SmetsWouters2007}}{Rewriting }}{equation.B.9}{}}
\newlabel{eq:7}{{15}{30}{\texorpdfstring {Rewriting \citet {SmetsWouters2007}}{Rewriting }}{equation.B.15}{}}
\newlabel{eq:8}{{16}{30}{\texorpdfstring {Rewriting \citet {SmetsWouters2007}}{Rewriting }}{equation.B.16}{}}
\newlabel{eq:9}{{17}{31}{\texorpdfstring {Rewriting \citet {SmetsWouters2007}}{Rewriting }}{equation.B.17}{}}
\newlabel{eq:10}{{18}{31}{\texorpdfstring {Rewriting \citet {SmetsWouters2007}}{Rewriting }}{equation.B.18}{}}
\newlabel{eq:11}{{19}{31}{\texorpdfstring {Rewriting \citet {SmetsWouters2007}}{Rewriting }}{equation.B.19}{}}
\newlabel{eq:12}{{20}{31}{\texorpdfstring {Rewriting \citet {SmetsWouters2007}}{Rewriting }}{equation.B.20}{}}
\newlabel{eq:13}{{21}{31}{\texorpdfstring {Rewriting \citet {SmetsWouters2007}}{Rewriting }}{equation.B.21}{}}
\bibstyle{plainnat}
\bibdata{dsges.bib}
\bibcite{Algoet-and-Cover-on-AEP}{{1}{1988}{{Algoet and Cover}}{{}}}
\bibcite{BlanchardKahn1980}{{2}{1980}{{Blanchard and Kahn}}{{}}}
\bibcite{Boivin-Giannoni-DSGEs-in-data-rich-environment}{{3}{2006}{{Boivin and Giannoni}}{{}}}
\bibcite{DeJongDave2007}{{4}{2011}{{DeJong and Dave}}{{}}}
\bibcite{Edge-Gurkaynak-on-dsges}{{5}{2010}{{Edge and Gurkaynak}}{{}}}
\bibcite{Andy-Fraser-on-HMMs}{{6}{2008}{{Fraser}}{{}}}
\bibcite{Geisser-predictive-sample-reuse}{{7}{1975}{{Geisser}}{{}}}
\bibcite{Geisser-Eddy-predictive-approach}{{8}{1979}{{Geisser and Eddy}}{{}}}
\bibcite{Gray-entropy-2nd}{{9}{2011}{{Gray}}{{}}}
\bibcite{Haavelmo-probability-approach}{{10}{1944}{{Haavelmo}}{{}}}
\bibcite{tEoSL-2nd}{{11}{2009}{{Hastie et~al.}}{{Hastie, Tibshirani, and Friedman}}}
\bibcite{Iskrev2009}{{12}{2010}{{Iskrev}}{{}}}
\bibcite{Jackson-Yariv-non-existence-of-representative-agents}{{13}{2017}{{Jackson and Yariv}}{{}}}
\newlabel{eq:14}{{22}{32}{\texorpdfstring {Rewriting \citet {SmetsWouters2007}}{Rewriting }}{equation.B.22}{}}
\bibcite{Kalman1960}{{14}{1960}{{Kalman}}{{}}}
\bibcite{Kirman-contra-representative-agent}{{15}{1992}{{Kirman}}{{}}}
\bibcite{Klein2000}{{16}{2000}{{Klein}}{{}}}
\bibcite{Komunjer-Ng-identification-of-DSGEs}{{17}{2011}{{Komunjer and Ng}}{{}}}
\bibcite{KydlandPrescott1982}{{18}{1982}{{Kydland and Prescott}}{{}}}
\bibcite{Lucas1976}{{19}{1976}{{Lucas}}{{}}}
\bibcite{Morgan-Winship-counterfactuals-2nd}{{20}{2015}{{Morgan and Winship}}{{}}}
\bibcite{Pearl-causality}{{21}{2000}{{Pearl}}{{}}}
\bibcite{Sims2002}{{22}{2002}{{Sims}}{{}}}
\bibcite{SmetsWouters2007}{{23}{2007}{{Smets and Wouters}}{{}}}
\bibcite{Spirtes-Glymour-Scheines-1st}{{24}{1993}{{Spirtes et~al.}}{{Spirtes, Glymour, and Scheines}}}
\bibcite{Stone1974}{{25}{1974}{{Stone}}{{}}}
\gdef \@abspage@last{33}
