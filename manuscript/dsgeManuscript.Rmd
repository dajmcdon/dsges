---
title: Empirical Macroeconomics and DSGE Modeling in Statistical Perspective


authors: 
  - name: Daniel J. McDonald
    address: |
      Department of Statistics  
      University of British Columbia  
      Vancouver, BC Canada  
      \email{daniel@stat.ubc.ca}
    thanks: Both authors were supported by a grant (INO1400020) from the Institute for New Economic Thinking.  We are grateful for advice and comments to David N. DeJong, J. Bradford DeLong and Robert Waldmann (none of whom should be held responsible for our choices).   DJM was partially supported by the National Science Foundation (grants DMS1407439 and DMS1753171) and the National Sciences and Engineering Research Council of Canada (RGPIN-2021-02618). CRS wishes to acknowledge support from the National Science Foundation (grants DMS1207759 and DMS1418124), and valuable conversations over many years with Zmarak M. Shalizi.
  - name: Cosma Rohilla Shalizi
    address: |
      Departments of Statistics and Data Science and of Machine Learning  
      Carnegie Mellon University  
      Pittsburgh, PA USA  
      _and_ Santa Fe Institute  
      Santa Fe, NM USA  
      \email{cshalizi@cmu.edu}
    
abstract: |
  Dynamic stochastic general equilibrium (DSGE) models have been an ubiquitous, and controversial, part of macroeconomics for decades.  In this paper, we approach DSGEs purely as statstical models.  We do this by applying two common model validation checks to the canonical \citet{SmetsWouters2007} DSGE: (1) we simulate the model and see how well it can be estimated from its own simulation output, and (2) we see how well it can seem to fit nonsense data.  We find that (1) even with centuries' worth of data, the model remains poorly estimated, and (2) when we swap series at random, so that (e.g.) what the model  gets as the inflation rate is really hours worked, what it gets as hours worked is really investment, etc., the fit is often only slightly impaired, and in a large percentage of cases actually _improves_ (even out of sample).  Taken together, these findings cast serious doubt on the meaningfulness of parameter estimates for this DSGE, and on whether this specification represents anything structural about the economy.  Constructively, our approaches can be used for model validation by anyone working with macroeconomic time series.
  
keywords: general equilibrium; model validation; time series; forecasting;



bibliography: dsges.bib
bibliographystyle: plainnat
output: 
  pdf_document:
    md_extensions: +raw_attribute
    fig_width: 6
    fig_height: 3.5
    fig_caption: true
    keep_tex: true
    template: template.tex
    includes:
      in_header: dsgeDefs.sty
---

# Introduction

Since the 1980s, academic macroeconomics has been dominated by dynamic
stochastic general equilibrium (DSGE) models, and economists have devoted a
much attention to their specification, elaboration, mathematical manipulation,
estimation, and theoretical refinement.  This rise to prominence has been
opposed, and there have always been critics of the whole approach on
theoretical grounds, charging that, in one way or another, DSGE models are (or
embody, or presume) bad economic theories.  Instead of questioning whether
DSGEs are good _economics_, we look at whether they are good _models_, i.e.,
whether they meet common, intuitive standards of statistical modeling.

In this paper, we are not primarily concerned with whether DSGE models can
predict macroeconomic variables outside of the time period used to estimate
their parameters.  Out-of-sample forecasting is a quite elementary test of any
statistical model's actual fit to the data \citep{tEoSL-2nd}, and this
has been appreciated for a very long time
\citep{Stone1974,Geisser-predictive-sample-reuse,Geisser-Eddy-predictive-approach}.
Moreover, it's well-established that DSGEs are bad at it
\citep{Edge-Gurkaynak-on-dsges}, and even vector autoregressions do much
better.  We thus take it for granted that DSGEs are _currently_ no good for
prediction, and ask whether they will _ever_ be capable of accurate forecasts.
By the simple, standard device of simulating a DSGE and then fitting the same
model to the simulation output, we show (sec.\ \ref{sec:simulate-estimate}
below) that even when correctly specified in their entirety and provided with
centuries of simulated data, these models remain incapable of forecasting.
Worse, their parameters remain very badly estimated.  To reliably estimate
models of such coelenterate flexibility would require thousands of years (at
least) of data from a stationary economy.

We are aware that many economists downplay using DSGEs for prediction on the
grounds that the models are instead supposed to inform us about the structure
of the economy and about the consequences of prospective policy interventions.
Even if this is true, however, one would need some reason to think that _this_
DSGE, rather than another, was getting things right.  While we agree that
models which are capable of accurate statistical prediction may be horrible at
causal, counter-factual prediction, it has long been understood that the
reverse is not true \citep{Spirtes-Glymour-Scheines-1st, Pearl-causality}, and
this is now literally a textbook point in causal inference
\citep{Morgan-Winship-counterfactuals-2nd}. A model which gets the causal
structure right, and can make accurate counter-factual predictions, should _a
fortiori_ be capable of accurate statistical prediction as well.  Hence
economists' confidence in their favorite DSGEs suitability for policy
evaluation cannot be rooted in their statistical predictive ability, since the
later does not exist.

We also undermine the notion that DSGE specifications capture the structure of
the economy by the simple expedient of swapping the different time series on
which they are fit --- giving the model as "investment" the series that is
really "hours worked", and so forth (sec.\ \ref{sec:permutation-summary}
below).  Not only does such series swapping do little to degrade the DSGE's
performance, in sample or out of sample, in a large fraction of permutations it
actually _enhances_ predictability.  It is, of course, open to an economist to
maintain their belief in a favorite DSGE's capturing the structure of the
macroeconomy even if it cannot predict, and cannot tell the difference between
the real data and one with all the series swapped, but such faith truly is
maintained on the evidence of things not seen.

The principles to which we appeal --- using simulation to assess estimation
methods; using randomization to gauge how well a model can appear to fit
nonsense data --- are not recondite points of mathematical theory, and involve
no controversial questions in the foundations of statistics.  Rather, they are
readily explained notions, accepted on pretty much all sides within modern
statistics, whose force is easily grasped once they are presented.  We return
to their implications for macroeconomic modeling in the conclusion.

## Brief Remarks on the Literature

There is a familiar story to the rise of DSGEs from the 1970s to the 1990s,
which roots them in, on the one hand, the desire to give "microfoundations" to
macroeconomic models, and, on the other hand, the \citet{Lucas1976} critique of
"old Keynesian" models, and the latter's apparent empirical failure in the
1970s.  The breakthrough paper, on this account, was the "real business cycle"
model of \citet{KydlandPrescott1982}.  Versions of this story are familiar from
textbooks (e.g., \citealt{DeJongDave2007}), and it's not our place here to
dispute it.  It is undeniably true that, following \citet{KydlandPrescott1982},
macroeconomists rapidly cultivated many breeds of DSGE model, embodying a range
of substantive economic assumptions, but also conserving certain crucial
features that define the lineage, such as the use of representative agents
(often just one representative agent) in equilibrium.  The motivation was also
conserved: DSGEs (it is held) clearly separate enduring, "structural" aspects
of the economy, ultimately relating to tastes, technologies and institutions,
from policy and from fluctuations.  The structural, tastes-and-technologies
aspects are held to be invariant, at least to policy and at least over the
relevant time scales.  Embedding these into a dynamic equilibrium model is
supposed to get around the Lucas critique, by ensuring that even if the
predictions of the model are known and are used as the basis of policy, the
model will remain valid, _because_ it describes an equilibrium over time.

The model we focus on, that of \citet{SmetsWouters2007}, hereafter denoted SW, while now some years
old, is still widely regarded as an acceptable baseline model.  A great deal of
macroeconomic theorizing thus consists of taking this model, or other very
similar ones, and elaborating on them by adding new frictions or shocks.  These
additions are generally intended to accommodate observed phenomena, to
incorporate conjectured mechanisms, or both.  It is not our purpose here to
comment on specific descendants of the SW or other baseline DSGE
models.  We merely note that the strategy of responding to issues or defects
with the model by always making it _more_ complex (more frictions, more shocks,
etc.) is at least questionable from a statistical perspective.

As we wrote in the introduction, our objective here is not to add to the
literature for or against DSGE models from the viewpoint of economic theory.
It is not even pertinent to survey that voluminous, often acrimonious,
literature.  We take no sides, here, on whether agents in macroeconomic models
should be assumed to have rational (rather than adaptive) expectations, should
have perfect (rather than bounded) rationality, should (or should not) obey the
Euler equations, etc.  We do not even take a side on whether aggregating large
numbers of heterogeneous economic actors into a few representative agents is in
fact "microfounded"[^microfound]. Those who are _a priori_ inclined to dismiss
DSGEs as inappropriate models will, we suspect, find our results on their
statistical difficulties congenial[^austrians].  But no result we are aware of
implies that theoretically sound economic models must also have good
statistical properties.  Moreover, for all we know at this stage, all the
proposed alternatives to DSGEs _also_ suffer from the same statistical flaws!
(We invite their partisans, or perhaps their enemies, to check.)


[^microfound]: But on this point, we cannot resist noting that it is hard to see how to get around the difficulties raised by \citet{Kirman-contra-representative-agent} and, more especially, \citet{Jackson-Yariv-non-existence-of-representative-agents}.

[^austrians]: Except, perhaps, for those who also object to quantitative empirical economics  in the first place.

Turning to the literature on statistical properties of DSGEs, this has mostly
focused on issues of estimation and testing; we cite specific works below as
needed.  A smaller body of work has examined out-of-sample forecasting ability
of DSGEs, notably \citet{Edge-Gurkaynak-on-dsges} (see also references
therein).  That paper found that these model predicted poorly, but ingeniously
attributed this to the close control exercises by central bankers and other
policy-makers.  (Other explanations suggest themselves.)  Another
tangentially-relevant body of work has examined identification issues in DSGEs.
The most important paper here is probably still
\citet{Komunjer-Ng-identification-of-DSGEs}, which gave fairly practical
algebraic conditions for checking the identifiability of DSGEs.  (This built
off earlier work by \citealt{Iskrev2009} among others.)  Identification means
that all model parameters can (in principle) be estimated in the limit;
estimates using the available, non-asymptotic amount of data might be
unacceptably imprecise, and might converge unacceptably slowly.  We are unaware
of any previous work which has addressed the practical estimability of DSGEs by
the means used here, or anything like it.  We are also unaware of any previous
work which probes whether DSGEs are actually capturing the structure of the
macroeconomy by anything like the expedient of series swapping.

\clearpage

# Specification and Baseline Estimation of the DSGE

In this section, we review how DSGE models work, the specification of the
\citet{SmetsWouters2007} DSGE that serves as our test case, and some of the
issues that arise in estimating the parameters of this model.

## Solving DSGEs {#sec:solveDSGEs}


A DSGE model is the solution to a constrained stochastic
inter-temporal optimization problem
\begin{align}
  z_t^* &=\argmax \sum_{t=0}^\infty \E g(z_t) &&\mbox{s.t.}&z_t&=h(z_{t-1}),
\end{align}
for some nonlinear functions $g$ and $h$ parametrized by a $k$-dimensional
vector of "deep" parameters $\theta$.  The economic agents posited by the
model are assumed to solve this problem (optimally) at each time $t$
conditional on all current and previously available information, and we
observe part of the solution: that is the observable data are $x_t$ which is a
subset of the indices of $z_t^*$.

To estimate a DSGE, the first step is to solve the optimization problem by
deriving the first-order conditions for an optimum.  The resulting nonlinear
system can be written as
\begin{equation}
  \Phi(z_t,\ z_{t+1})=0,
\end{equation}
for some $n$-dimensional function $\Phi$.  Such systems can rarely be solved
analytically for the optimal path; instead, the first-order conditions are used
to express the model in terms of stationary variables, and the system is
linearized around this steady-state (that is, let $z$ be the vector satisfying
$z=z_t=z_{t-1}$). Upon writing $z_t = z^*_t - z$, the (linearized) DSGE can be
written as
\begin{equation}
  \Gamma_0 z_t = \Gamma_1 z_{t-1} + A + B \epsilon_t + C \eta_t
  \label{eq:lin-dsge}
\end{equation}
in the notation of \citet{Sims2002}. Here $\epsilon_t$ are exogenous, possibly
serially correlated, random disturbances and $\eta_t=z_t - \E_t z_{t+1}$ are
expectational errors determined as part of the model solution. The matrices
$\Gamma_0$, $\Gamma_1$, $A$, $B$, and $C$ are functions of $\theta$.

There are a number of ways to estimate $\theta$ using data; a complete
treatment is beyond our scope here, but see \citet{DeJongDave2007} for details.
We will focus on likelihood-based approaches, and so we must solve the model in
equation \eqref{eq:lin-dsge}.  There are many approaches to solving linear rational
expectations models \citep[e.g.][]{BlanchardKahn1980,Klein2000}, but we will
use that in \citet{Sims2002} due to its ubiquity.  This method essentially uses
a QZ factorization to remove $\Gamma_0$ from the left side of the equation
while correctly handling explosive components of the model (merely multiplying through
by the generalized inverse of $\Gamma_0$, $\Gamma_0^\dagger$, can lead to
portions of the product $\Gamma_0^\dagger \Gamma_1$ implying nonstationarity).
Following this procedure, we retrieve a system of the form
\begin{equation}
  \label{eq:ss-system}
  z_t = d + T z_{t-1} + H \varsigma_t
\end{equation}
as long as there is a unique mapping from equation \eqref{eq:lin-dsge} (there may be
multiple solutions or none depending on $\theta$). Since some of the $z_t$ are
unobserved, we augment the transition equation in \eqref{eq:ss-system} with an
observation equation
\begin{equation}
  \label{eq:obs-equation}
  x_t = Z z_t,
\end{equation}
where the matrix $Z$ subselects the appropriate elements of $z_t$.  Collecting
equations \eqref{eq:ss-system} and \eqref{eq:obs-equation} gives the form of a linear state-space
model.  Assuming that the errors $\varsigma_t$ are serially independent multivariate
Gaussian allows us to evaluate the likelihood of some parameter vector $\theta$
given observed data $x_1,\ldots,x_T$.  Evaluating the likelihood can be done
using the Kalman filter \citep{Kalman1960} which is readily available in most
software packages. The procedure outlined above is summarized in
\autoref{alg:solveRE}.
\begin{algorithm}[t]
\begin{algorithmic}[1]
  \State Write down DSGE as a constrained optimization problem.
  \State Determine the first order conditions for an optima.
  \State Determine steady state path of observables.
  \State Linearize the DSGE around the steady-state yielding a general
    form for \eqref{eq:lin-dsge}.
  \Procedure{Estimate the model}{$\theta$}
    \State Fix a plausible parameter vector $\theta$.
    \State For this $\theta$, cast \eqref{eq:lin-dsge} into state-space form
      using the method of \citet{Sims2002}.
    \State Using equations \eqref{eq:ss-system} and \eqref{eq:obs-equation}, evaluate the
      likelihood of $\theta$ using the Kalman filter.
    \State Maximize the likelihood or explore the posterior, repeating
      as needed at new values of $\theta$.
    \EndProcedure
  \end{algorithmic}
  \caption{Pseudoalgorithm for estimating linear rational expectations models}
  \label{alg:solveRE}
\end{algorithm}



\subsection{Estimating the \citet{SmetsWouters2007} model}
\label{sec:estim-sw-model}

```{r knitr-setup, include=FALSE}
library(knitr)
library(kableExtra)
library(tidyverse)
library(QZ)
library(FKF)
library(lubridate)
library(scales)

opts_chunk$set(
  message = FALSE, warning = FALSE, cache = TRUE, 
  autodep = TRUE, echo = FALSE,
  include = TRUE, size = "small",
  fig.align = 'center', fig.width = 6, fig.height = 4,
  fig.path = "gfx/", 
  fig.pos = "t"
)
theme_set(theme_bw(base_family = "Times", base_size = 12))
green = '#00AF64'
blue = '#0B61A4'
red = '#FF4900'
orange = '#FF9200'
colvec = c(green, blue, red, orange)
trainset = 1:200
```


```{r load-dsge-data}
perm_res <- read_rds(here::here("update2022", "permutation_results.rds"))
load(here::here("data", "SWdataUpdated.Rdata")) # named y
n <- ncol(y)
testset <- seq(y)[-trainset]
source(here::here('code', 'functions', 'modelsol.R'))
source(here::here('code', 'functions', 'reparlogp.R'))
source(here::here('code', 'functions', 'SimsCodesFort.R'))
source(here::here('code', 'priorsetup.R'))
long_run_pars <- read_rds(here::here("update2022", "swlong-run-pars.rds"))
#load(here::here("cluster_output", "SWlong.Rdata"))
#source('../cluster_output/SWlong.Rdata')
perms = perm(7, 7)
SWPerm = perms[1, ]
truePerm = 1
pvec <- perm_res$par_ests[truePerm, ]
unPerm <- map(split(perms, seq(nrow(perms))), ~ match(1:7, .x))
series = rownames(y)
# unscramble the permuted test errors
testerr <- map2(split(perm_res$test_mse, seq(nrow(perms))), unPerm, ~ .x[.y]) %>%
  do.call("rbind", .) %>%
  as_tibble(.name_repair = "unique")
series_names <- c('hours worked', 'interest rate', 'inflation', 
                   'output', 'consumption', 'investment', 'wages') 
names(testerr) = series_names
testerr$permutation = seq(nrow(perms))
testerr <- testerr %>% pivot_longer(-permutation)
baseline <-  testerr %>% 
  filter(permutation == truePerm) %>% 
  select(-permutation) %>%
  rename(baseline = value)
logperc <- left_join(testerr, baseline, by = "name") %>%
  mutate(value = log(value) - log(baseline)) %>%
  select(-baseline)

nperms = nrow(perms)
sw_pvec = c(.45,.24,.52,.45,.24,.14,.24,.95,.18,.97,.71,.12,.9,.97,.74,
            .88, 5.48,1.39,.71,.73, 1.92,.65,.59,.22,.54,1.61,2.03,.81,.08,
            .22,.81,.16,-.1,.43, .52,.19)
parmGreek = c('sigma[a]','sigma[b]','sigma[g]','sigma[I]', 'sigma[r]', 
              'sigma[p]','sigma[w]',
              'rho[a]', 'rho[b]', 'rho[g]', 
              'rho[I]', 'rho[r]', 'rho[p]','rho[w]', 'mu[p]',
              'mu[w]', 'phi1', 'sigma[c]', 'h', 'xi[w]', 'sigma[l]', 
              'xi[p]', 'iota[w]',
              'iota[p]', 'Psi', 'Phi', 'r[pi]', 'rho', 'r[y]', 
              'r[Delta][y]', 'bar(pi)', 
              '100(beta^{-1} -1)', 'bar(l)', 'bar(gamma)', 'rho[ga]', 'alpha')
```

In this section, we provide a description of the procedure we use for
estimating the \citet{SmetsWouters2007} model. The model is now standard in the
macroeconomic forecasting literature, and, though code is readily available
(for example in \texttt{Dynare} or \texttt{Matlab}\footnote{See also
  \url{https://www.aeaweb.org/articles.php?doi=10.1257/aer.97.3.586&fnd=s}}),
our version is implemented fully in \texttt{R}. A complete description of the
economic implications of the model and its log-linearized form can be found in
\citet{SmetsWouters2007} as well as \citet{Iskrev2009} and other sources.

For this model, we use seven observable data series: output growth,
consumption growth, investment growth, real wage growth, inflation,
hours worked, and the nominal interest rate, which we collect into the
vector $x_t$
\begin{equation}
  x_t = \begin{bmatrix} y_t-y_{t-1} & c_t - c_{t-1} & i_t-i_{t-1} &
    w_t-w_{t-1} & \pi_t & l_t & r_t \end{bmatrix}^\top.
\end{equation}
We describe the specific data and preprocessing routines in
\autoref{sec:data-preprocessing}, but we note that all the data
are publicly available from the Federal Reserve
Economic Database
(\href{http://research.stlouisfed.org/fred2/}{FRED}). We use data from
the first quarter of 1956  until the fourth quarter of 2018. Following
preprocessing, we are left with `r ncol(y)` available time points.


The model has 52 "deep" parameters as well as 7 parameters representing the
standard deviations of the stochastic shocks. Of these 59 total parameters, we
estimate 36: 18 are derived from steady-state values as functions of other
parameters and 5 are fixed a priori (as in \citealt{SmetsWouters2007}). The
prior distributions for the 36 estimated parameters are given in
\autoref{tab:parameter-estimate-table}. To estimate the model we minimize the
negative log likelihood, penalized by the prior.  This is the same as finding
the maximum _a posteriori_ estimate in a Bayesian setting.  Because the
likelihood is ill-behaved, having many flat sections as well as local minima,
we used \texttt{R}'s \texttt{optimr} package. We estimated the parameter using
both the simulated annealing method, which stochastically explores the
likelihood surface in a principled manner, and the conjugate gradient
technique.  Each procedure was started at 5 random initializations (drawn from
the prior distribution) and run for 50,000 iterations (likelihood evaluations)
for each starting point.  We train the model using only the first
`r length(trainset)` time points, saving the remainder to evaluate the model's
(pseudo-out-of-sample) predictive performance.

```{r}
our_llike <- getlogLike(repar(pvec, prior, TRUE)$outparv, 
                        y[,trainset], prior)
sw_llike <- getlogLike(repar(sw_pvec, prior, TRUE)$outparv, 
                        y[,trainset], prior)
long_llike <- getlogLike(repar(long_run_pars, prior, TRUE)$outparv, 
                        y[,trainset], prior)
```

\autoref{tab:parameter-estimate-table} presents the posterior mode based on our
procedure. Note first that some of the parameter estimates are similar to those
presented in \citet{SmetsWouters2007} (shown in the last column), while others
differ dramatically.  However, comparing the likelihood of of our estimated
parameters to those in \citet{SmetsWouters2007}, our fit is significantly
better.  For our dataset, the penalized negative log likelihood of the
parameters is `r round(our_llike, 0)` compared to
`r round(sw_llike, 0)`, an
improvement of more than
`r round(1 - our_llike/sw_llike, 3)*100`%.
The result is similar for the unpenalized negative log likelihood.  To check
for robustness, we also ran the optimization procedure for 1 million parameter
draws and the likelihood only decreased by less than 0.001\%.  We are therefore
confident that we have a parameter combination which nearly achieves the global
optimum.[^usvsSW]


[^usvsSW]: We have been unable to explain why \citet{SmetsWouters2007} gives such a different estimate of the posterior mode. It is worth pointing out however, that their MCMC is not necessarily attempting to find the maximum of the posterior, although it may locate one.
  

```{r parameter-estimate-table, results='asis'}
parameter_latex = c(
    '$\\sigma_a$','$\\sigma_b$','$\\sigma_g$','$\\sigma_I$', 
    '$\\sigma_r$', '$\\sigma_p$','$\\sigma_w$',
    '$\\rho_a$', '$\\rho_b$', '$\\rho_g$', 
    '$\\rho_I$', '$\\rho_r$', '$\\rho_p$','$\\rho_w$', '$\\mu_p$',
    '$\\mu_w$', '$\\phi_1$', '$\\sigma_c$', '$h$', '$\\xi_w$', '$\\sigma_l$', 
    '$\\xi_p$', '$\\iota_w$',
    '$\\iota_p$', '$\\Psi$', '$\\Phi$', '$r_\\pi$', '$\\rho$', '$r_y$', 
    '$r_{\\Delta y}$', '$\\overline{\\pi}$', 
    '$100(\\beta^{-1} -1)$', '$\\overline{l}$', '$\\overline{\\gamma}$',
    '$\\rho_{ga}$', '$\\alpha$')
tab = tibble(
  parameter = parameter_latex,
  dist = prior$dist, 
  mean = prior$m,
  std = prior$sd, 
  lb = round(prior$lb, 2), 
  ub = prior$ub,
  mode = round(pvec, 2), 
  SWmode = sw_pvec)
kable(tab, booktabs = TRUE, digits = 2, escape = FALSE, 
  caption = "Prior distributions, posterior modes, and posterior mode as
      estimated by \\citet{SmetsWouters2007} for the 36 estimated
      parameters. All values are rounded to two decimal places.",
  align = "c",
  col.names = linebreak(
    c("",
      "prior\ndistribution", "prior\nmean", "prior\nstdev",
      "lower\nbound", "upper\nbound", "posterior\nmode",
      "SW posterior\nmode"), align = "c")) %>%
  kable_styling(font_size = 9)
```

\clearpage

# Simulate and estimate {#sec:simulate-estimate}

A simple way to evaluate a stochastic model is to simulate a long time series
from the model and see how well we can estimate the generating process given
more and more data.  In a sense, this is a minimal requirement for any model:
does it produce consistent estimates of the true parameters?  And furthermore,
how much data will we need?  Statistical theory for independent and identically
distributed data says that maximum likelihood estimators for a fixed number of
parameters are $n$-consistent in MSE.  This is essentially as fast as we could
hope.  In this exercise, we abstract from the non-linear DSGE: supposing that
the data were actually generated by the linearized DSGE represented by
equations \eqref{eq:ss-system} and \eqref{eq:obs-equation}, can we recover the
"deep" parameters which generated the data?

To answer this (relatively simple) question, we generate data using the
parameters estimated above and presented in
\autoref{tab:parameter-estimate-table}. To reduce dependence on initial
conditions, we simulate 3100 data points and discard the first 1000.  We then
train the model using the first 100 data points (25 years worth of quarterly
data), and try to predict the next 1000 to measure out-of-sample performance.
We then increment forward 20 time-steps (5 years), train again and predict the
next 1000 observations. We continue this process until we are using 1100 data
points (275 years of data) to estimate the model. To average over simulation
error, we repeat the entire exercise 100 times. For each estimation, we
initialize the optimization procedure at the true parameter value and run it
for 30000 iterations. This ensures that the true parameters are considered,
maximizing the chance that the estimation will return the data generating
values. Our goal here is to learn whether we could hope to learn the right
parameters in 275 years' time if the linearized DSGE is actually true, and
furthermore, how well can we predict future economic movements under this
idealized scenario.[^industrev]

[^industrev]: It is, of course, wildly implausible to imagine a 275 year interval which does _not_ see massive changes to tastes, technologies and institutions. The differences between 1746 and 2021 are obvious, but your favorite economic historian will explain at length that these were all transformed between 1471 and 1746, too.  Recognizing this hardly makes DSGEs _more_ attractive.

All figures in this section show the mean in red along with 50%, 75%, and
90% confidence bands calculated from the replications.

```{r}
simest <- read_rds(here::here("update2022", "simulate_estimate.rds"))
scales <- simest %>% select(job.id, nestim, ends_with("_sc")) %>%
    rename_with(.cols = ends_with("_sc"), ~ sub("[_]sc$", "", .)) %>%
    pivot_longer(
      -c(job.id, nestim), 
      names_to = "series", values_to = "scaling")
```

```{r plot-ribbons-fun}
plot_ribbons <- function(dat, col, centre = c("median", "mean"), 
                         probs = c(.95, .8, .5)) {
  centre <- match.arg(centre, several.ok = TRUE)
  line_cols <- c(mean = orange, median = "black")
  dat <- dat %>%
    group_by(nestim, .add = TRUE) %>%
    summarise(
      itvals = list(ggdist::median_qi({{ col }}, .width = probs)),
      mean = mean({{ col }})
    ) %>%
    unnest(itvals)
  g <- dat %>% 
    ggplot(aes(x = nestim)) +
    ggdist::geom_lineribbon(aes(ymin = ymin, ymax = ymax)) +
    scale_fill_brewer() +
    scale_x_continuous(expand = expansion(), breaks = 1:4 * 250)
  
  if ("median" %in% centre) 
    g <- g + geom_line(aes(y = y), color = line_cols["median"])
  
  if ("mean" %in% centre)
    g <- g + geom_line(aes(y = mean), color = line_cols["mean"])

  return(g + xlab("Number of training observations"))
}
```

```{r train-error, fig.cap="Training (in sample) error averaged across series as the number of training points increases. We would expect the variability to decline while the average remains constant. The median is shown in black along with 50\\%, 80\\%, and 95\\% confidence bands calculated over the replications."}
scaled_train_error <- full_join(
  scales,
  simest %>% select(job.id, starts_with("mste_")) %>%
    rename_with(.cols = starts_with("mste_"), ~ sub("^mste[_]", "", .)) %>%
    pivot_longer(-job.id, names_to = "series", values_to = "mste"),
  by = c("job.id", "series"))

scaled_train_error %>%
  mutate(mste = mste / scaling) %>%
  group_by(job.id) %>%
  summarise(mste = mean(mste), nestim = mean(nestim)) %>%
  ungroup() %>%
  plot_ribbons(mste, "median") +
  ylab('Scaled in-sample MSE')
```

```{r test-error, fig.cap="Out-of-sample error averaged across series as the number of training points increases. We would expect both the variability and the average to decline. The orange line is the test error for the true (oracle) model."}
scaled_test_error <- full_join(
  scales,
  simest %>% select(job.id, starts_with("mspe_")) %>%
    rename_with(.cols = starts_with("mspe_"), ~ sub("^mspe[_]", "", .)) %>%
    pivot_longer(-job.id, names_to = "series", values_to = "mspe"),
  by = c("job.id", "series"))

scaled_oracle_error <- full_join(
  scales,
  simest %>% select(job.id, starts_with("base_")) %>%
    rename_with(.cols = starts_with("base_"), ~ sub("^base[_]", "", .)) %>%
    pivot_longer(-job.id, names_to = "series", values_to = "oracle"),
  by = c("job.id", "series"))

mean_oracle_error <- scaled_oracle_error %>%
  mutate(oracle = oracle / scaling) %>%
  group_by(job.id) %>%
  summarise(oracle = mean(oracle), nestim = mean(nestim)) %>%
  ungroup() %>%
  group_by(nestim) %>%
  summarise(mean = mean(oracle), median = median(oracle))

scaled_test_error %>%
  mutate(mspe = mspe / scaling) %>%
  group_by(job.id) %>%
  summarise(mspe = mean(mspe), nestim = mean(nestim)) %>%
  ungroup() %>%
  plot_ribbons(mspe, "median") +
  coord_cartesian(ylim = c(0.4, 0.7)) +
  geom_line(data = mean_oracle_error, aes(y = mean), color = orange) +
  ylab('Scaled out-of-sample MSE')
```


\autoref{fig:train-error} shows the average training error (averaged across the
7 series).  As we would expect, the variability declines as the size of the
training set increases, though not the average.  \autoref{fig:test-error} shows
the average prediction error over the 7 series.  It improves markedly as the
training set increases to about 400 observations ($=100$ years) but then
plateaus.  This is troubling: as we get more and more data, we can not predict
new data any better.  This indicates one of three possibilities: (1) that with
about 400 observations, we can estimate the parameters nearly perfectly, (2)
that the model is poorly identified---some parameters will simply never be well
estimated, but we can predict well anyway, or (3) the data are so highly
correlated that the range of training observations we consider is far too
small---we actually need millions of observations in order to see a meaningful
decline in out-of-sample predictive performance.  We can better determine which
of these three is occurring by comparing with the predictive performance of the
true parameters and examining the error in parameter estimates. The blue line
in \autoref{fig:test-error} is the out-of-sample mean prediction error for the true parameters. The test error is not
getting any closer to this ideal scenario, plateauing slightly above the
baseline by about 400 training points.  This seems to suggest that explanation
(2) is accurate: even with more data, we will never be able to recover
the true parameters, though we get some improvement in predictions relatively
quickly.

```{r parm-error, fig.cap="The parameter MSE shows similar behavior to the predictive MSE: steep initial decline toward an asymptote greater than that of the true model."}
tib_parm <- tibble(parm = parmGreek, true_parm = pvec)
parm_error <- simest %>%
  select(job.id, nestim, `sigma[a]`:alpha) %>%
  pivot_longer(-c(job.id, nestim), names_to = "parm") %>%
  left_join(tib_parm, by = "parm")
  
msparm_error <- parm_error %>%
  mutate(value = (value - true_parm)^2) %>%
  group_by(job.id) %>%
  summarise(nestim = mean(nestim), msparm = mean(value))

plot_ribbons(msparm_error, msparm, "median") +
  geom_hline(yintercept = 0, color = orange) +
  ylab('Parameter MSE')
```

```{r pen-negll, fig.cap="The negative log-likelihood (in-sample) per observation."}
simest %>%
  select(nestim, lltrain) %>%
  mutate(lltrain = lltrain / nestim) %>%
  plot_ribbons(lltrain, "median") +
  ylab('(penalized) negative log-likelihood (training data)')
```

```{r entropy-limit, eval=FALSE}
n <- 1e6
long <- generate(n, pvec)
filt <- getFilterOutput(long, pvec)
Zt <- filt$ssmats$Zt
GG <- filt$ssmats$GG
Pbar <- filt$Pt[,,n + 1]
Plast <- filt$Pt[,,n]
rm(filt)
frobenius_norm <- sqrt(sum((Pbar - Plast)^2))
limit_obs_var <- Zt %*% Pbar %*% t(Zt) + GG
entropy_rate <- determinant(2 * pi * exp(1) * limit_obs_var)$modulus / 2
saveRDS(entropy_rate, here::here("update2022", "entropy_rate.rds"))
```

```{r load-entropy-rate}
entropy_rate <- read_rds(here::here("update2022", "entropy_rate.rds"))
```


```{r predict-negll, fig.cap="The negative predictive log-likelihood (out-of-sample) per observation. The horizontal line is the theoretical limit."}
simest %>%
  select(nestim, llpred) %>%
  mutate(llpred = -llpred / (2100 - nestim)) %>%
  plot_ribbons(llpred, "median") +
  geom_hline(yintercept = entropy_rate, color = orange) + 
  coord_cartesian(ylim = c(5, 10)) +
  ylab('Predictive log-likelihood (testing data)')
```

```{r individual-series, fig.cap="Mean-squared prediction error for each series."}
left_join(
  scaled_test_error %>% select(-scaling),
  scaled_oracle_error %>% select(-scaling),
  by = c("job.id", "nestim", "series")) %>%
  mutate(
    series = recode(
      series, dc = "consumption", dinve = "investment", 
      dw = "wages", dy = "output", labobs = "hours worked", 
      pinfobs = "inflation", robs = "interest rate"),
    mspe = log(mspe / oracle) * 100
  ) %>%
  group_by(series) %>%
  plot_ribbons(mspe, "median") + 
  facet_wrap(~ series) + 
  coord_cartesian(ylim = c(0, 25)) +
  geom_hline(yintercept = 0, color = orange) +
  ylab('100 x log(MSE / oracle)')
```

Examining the parameter error serves to confirm this conjecture.
\autoref{fig:parm-error} shows the average squared error of the parameter
estimates.  Improvement in this metric also stagnates despite the growing
training sets. \autoref{fig:pen-negll} and \autoref{fig:predict-negll} show the
change in information per observation (in sample) and per prediction (out of
sample).  For maximum likelihood inference, we would expect both of these to
decline as the amount of training data increases.  These figures therefore
confirm that the estimation procedure is responding to the increased sample
size even though prediction and estimation errors both fail to improve
meaningfully.  \autoref{fig:individual-series} shows the average prediction
error for each series individually. From this decomposition, prediction of all
time series improves, but only labor ever reaches the minimum (relative to the
truth).  The other series are all predicted poorly.



```{r shock-sds, fig.cap = "Estimates for the standard deviations of stochastic shocks."}
shocks <- parmGreek[1:7]
parm_error %>% 
  filter(parm %in% shocks) %>%
  group_by(parm) %>%
  plot_ribbons(value, "median") +
  facet_wrap(~ parm, scales = "free_y", labeller = label_parsed) +
  geom_hline(
    data = tib_parm %>% filter(parm %in% shocks), 
    mapping = aes(yintercept = true_parm), color = orange) + 
  ylab('')
```

```{r autocorrelations-shocks, fig.cap="Estimates for the autocorrelation parameters in the shock processes."}
autocorrelations <- parmGreek[8:14]
parm_error %>% 
  filter(parm %in% autocorrelations) %>%
  group_by(parm) %>%
  plot_ribbons(value, "median") +
  facet_wrap(~ parm, scales = "free_y", labeller = label_parsed) +
  geom_hline(
    data = tib_parm %>% filter(parm %in% autocorrelations), 
    mapping = aes(yintercept = true_parm), color = orange) + 
  ylab('')
```

```{r deep1, fig.cap='Estimates of "deep" parameters'}
deep <- parmGreek[15:22]
parm_error %>% 
  filter(parm %in% deep) %>%
  group_by(parm) %>%
  plot_ribbons(value, "median") +
  facet_wrap(~ parm, scales = "free_y", labeller = label_parsed) +
  geom_hline(
    data = tib_parm %>% filter(parm %in% deep), 
    mapping = aes(yintercept = true_parm), color = orange) + 
  ylab('')
```

```{r deep2, fig.cap='Estimates of "deep" parameters'}
deep <- parmGreek[23:30]
parm_error %>% 
  filter(parm %in% deep) %>%
  group_by(parm) %>%
  plot_ribbons(value, "median") +
  facet_wrap(~ parm, scales = "free_y", labeller = label_parsed) +
  geom_hline(
    data = tib_parm %>% filter(parm %in% deep), 
    mapping = aes(yintercept = true_parm), color = orange) + 
  ylab('')
```

```{r deep3, fig.cap='Estimates of "deep" parameters'}
deep <- parmGreek[31:36]
parm_error %>% 
  filter(parm %in% deep) %>%
  group_by(parm) %>%
  plot_ribbons(value, "median") +
  facet_wrap(~ parm, scales = "free_y", labeller = label_parsed) +
  geom_hline(
    data = tib_parm %>% filter(parm %in% deep), 
    mapping = aes(yintercept = true_parm), color = orange) + 
  ylab('')
```

Figures \ref{fig:shock-sds}--\ref{fig:deep3} show the estimates of each
parameter individually.  The horizontal orange lines indicate the truth. Here we
can see that while many parameters are well-estimated with decreasing
variability as the training sets grow, others converge to the wrong values, or
fail to converge at all.  This phenomenon holds especially for both of the
parameters of the shock processes, but also for some of the
economically-relevant "deep" parameters.  
For instance, $\sigma_l$, the
elasticity of labor supply with respect to the real wage, is consistently
underestimated by about `r round(1 - filter(parm_error, parm=="sigma[l]", nestim==1000) %>% summarise(mean(value)) %>% pull() / pvec[parmGreek == "sigma[l]"] * 100, 0)`\%.  
The data
provides essentially no information about $\iota_w$, which measures the
dependence of real wages on lagged inflation.  Other parameters which are
poorly estimated include $\varphi$, the steady-state elasticity of
the capital adjustment cost function.  In all these cases, estimation is
biased, so using the estimated values from the real data to draw conclusions
about the real economy is unwise.  It is possible that this bias is due to the
linearization procedure[^bias], in which case, not only should linearized models be
avoided for prediction, but also for drawing any economic inferences.  Overall,
most parameters are poorly identified as evidenced by their stable (rather than
decreasing) variability with more data.

[^bias]: It is possible, though not all that plausible, that a misspecified linearized model might be _unbiased_ for these parameters inside the original nonlinear model.

To examine case (3)---that the data is so correlated that even 250 years is too little to produce accurate estimates of the true parameters---we may ask how closely the information per observation displayed in \autoref{fig:pen-negll} approaches the theoretical asymptotic limit.\footnote{We are adapting a procedure recommended by \citet{Andy-Fraser-on-HMMs} for evaluating general hidden Markov models.}  For essentially any stationary ergodic stochastic process, the limit of the negative loglikelihood per observation as the number of observations approaches infinity exists and is unique \citep{Gray-entropy-2nd}. This limit is the _entropy rate_ of the process. Mathematically, with probability~1,
\[
\lim_{n\rightarrow\infty} -\frac{1}{n}\ell(X_{1:n}; \theta_0) = h(\theta_0),
\]
where $h(\theta_0)$ is the entropy rate. Furthermore, by the generalized asymptotic equipartition property \citep{Algoet-and-Cover-on-AEP}, one can show that for any other parameter vector $\theta \neq \theta_0$, with probability 1,
\[
\lim_{n\rightarrow\infty} -\frac{1}{n}\ell(X_{1:n}; \theta) = h(\theta_0) + \mathrm{KL}(\theta_0\ ||\ \theta),
\]
where the second term is the Kullback-Leibler divergence rate \citep{Gray-entropy-2nd}. Because $\mathrm{KL}(\theta_0\ ||\ \theta) \geq 0$, asymptotically, $h(\theta_0)$ is a lower bound for the average negative loglikelihood of any parameter vector.

Because the data is generated from a linear Gaussian state space model, it is not hard to calculate that $h(\theta_0) \approx$ `r round(entropy_rate, 2)`, significantly \emph{larger} than any of the values in \autoref{fig:pen-negll}. Somehow, the estimated parameters generally have \emph{lower} negative loglikelihoods than the true parameter. 

The fact that the standard deviations of the shocks (\autoref{fig:shock-sds}) are consistently underestimated suggests that, even over this period, the in-sample fit to the data gives an overly optimistic picture of the long run behavior: we have not allowed $n$ to get large enough. \autoref{fig:entropy-investigation} shows the negative loglikelihood per observation over a much longer horizon---up to 75,000 years of quarterly data. For thousands of years, the parameter vector estimated over the first 250 years is less than the theoretical lower bound, but eventually, it starts to curve upward. This suggests that, both explanations (2) and (3) are accurate: in order to estimate a DSGE accurately, we need massive amounts of data.

```{r entropy-investigation, fig.cap = "Comparing the in-sample negative loglikelihood per observation of a parameter estimated with 1000 observations relative to the true parameter. In the long-run, the entropy rate is a lower bound, but only after about 12,000 years of quarterly data."}
tib <- readRDS(here::here("update2022","entropy-investigation-long.rds")) 
ggplot(tib, aes(n, neg_loglikelihood, color = name)) +
  geom_line() + 
  ylab("negative loglikelihood per observation") +
  xlab("number of observations (log scale)") +
  scale_x_log10(labels = scales::label_number_si()) + 
  geom_point() + 
  geom_hline(yintercept = entropy_rate, color = orange) +
  scale_color_viridis_d(
    begin = .25, end = .75, 
    labels = expression(estimated~hat(theta), true~theta[0])) +
  theme(legend.title = element_blank(), legend.position = "bottom")
```

\clearpage

# Permuting the data {#sec:permutation-summary}


A second way to assess the predictive ability (and economic content) of the
SW DSGE model is to perform a simple permutation test,
permuting _across_ the series rather than within them.  That is, rather than
giving the model data in the order it expects, we swap the data series with
each other and see if the model predicts future data any better.  We estimated
the model on the properly ordered data (presented in the previous section) as
well as all `r nrow(perms)-1` other permutations of the `r nrow(y)` data
series. For each estimation, we used the same estimation procedure as before to
minimize the (penalized) negative log likelihood. The model is trained using
the first `r length(trainset)` time points and its predictive performance is
tested on the remaining `r length(testset)`.

```{r mean-percent-improvement, fig.cap="Average percentage improvement in out-of-sample forecast MSE. The horizontal orange line represents baseline performance. Lower values are better."}
df <- logperc %>%
  group_by(permutation) %>%
  summarise(value = mean(value)) %>%
  arrange(value) %>%
  mutate(x = 1:nperms)

imp = round(mean(df$value < 0) * 100, 1)
ggplot(df, aes(x, value)) + 
  geom_point(color = blue) + 
  geom_hline(yintercept = 0, color = orange) + 
  ylab(bquote(log("permuted MSE") - log("baseline MSE"))) + 
  xlab(paste0(imp,"% of permutations improved"))
```

The next few figures summarize the results. We report three criteria for
measuring relative performance. The first is the percent improvement in
(out-of-sample) mean-squared test error (MSE) calculated as the natural
logarithm of the test error for a particular permutation divided by that of the
baseline model and then averaged across all `r nrow(y)` series:
\begin{equation}
  \mbox{Average Percent MSE Improvement (p)} = \frac{1}{7} \sum_{i=1}^7 \log
  \left(\frac{\sum_{t=201}^{251} (\hat{x}_{it}^{(p)}-x_{it})^2}
    {\sum_{t=201}^{251} (\hat{x}_{it}^{b}-x_{it})^2}\right),
\end{equation}
where a superscript $b$ represents the baseline model and $(p)$ the $p^{th}$
permutation.  The result is shown in \autoref{fig:mean-percent-improvement}. \autoref{fig:series-percent-improvement}
shows boxplots for the percentage improvement separately for each time
series. The best model had an average percentage improvement of `r -round(min(df$value), 1) * 100`\% 
relative to the baseline model. The vertical line at zero represents performance
equivalent to baseline. About `r round(mean(df$value < 0) * 100, 0)`\% of permutations had better predictive performance than the baseline model.

```{r series-percent-improvement, fig.cap="Percentage improvement in out-of-sample forecast MSE for each data series individually. The horizontal orange line represents baseline performance. Values to the left are better."}
percent_better_models <- round(mean(df$value < 0) * 100, 0)
logperc %>% 
  ggplot(aes(name, value)) + 
  geom_violin(aes(fill = name)) + 
  geom_boxplot(width = 0.1, outlier.shape = NA) +
  coord_flip() + 
  geom_hline(yintercept = 0) + 
  scale_fill_viridis_d(guide = "none") +
  ylab(bquote(log("permuted MSE") - log("baseline MSE"))) +
  xlab('')
```

```{r mean-scaled-mse, fig.cap="Average improvement in out-of-sample forecast MSE scaled by the variance of the data. The horizontal orange line represents baseline performance."}
yvar <- t(apply(y, 1, var))
colnames(yvar) <- series_names
yvar <- as_tibble(yvar) %>% 
  pivot_longer(everything(), names_to = "name", values_to = "var")
sc_test_err <- left_join(testerr, yvar, by = "name") %>%
  mutate(value = value / var, var = NULL)

df <- sc_test_err %>%
  group_by(permutation) %>%
  summarise(value = mean(value)) %>%
  arrange(value) %>%
  mutate(x = 1:nperms)

sw_sc_test_err <- df %>% filter(permutation == 1) %>% pull(value)


imp = round(mean(df$value < sw_sc_test_err) * 100, 1)
ggplot(df, aes(x, value)) + 
  geom_point(color = blue) + 
  geom_hline(yintercept = sw_sc_test_err, color = orange) + 
  ylab('mean MSE scaled by series variance') + 
  xlab(paste0(imp,"% of permutations improved"))
```

```{r series-scaled-mse, fig.cap="Improvement in out-of-sample forecast MSE scaled by the variance of the data for each data series individually. The orange dots represents baseline performance."}
sc_test_err %>% 
  ggplot(aes(name)) + 
  geom_violin(aes(y = value, fill = name)) + 
  geom_boxplot(aes(y = value), width = 0.1, outlier.shape = NA) +
  geom_crossbar(
    data = sc_test_err %>% filter(permutation == 1L), 
    mapping = aes(y = value, ymin = value, ymax = value)) +
  # geom_point(data = sc_test_err %>% filter(permutation == 1L)) +
  coord_flip() + 
  scale_fill_viridis_d(guide = "none") +
  ylab('MSE scaled by series variance (log scale)') + 
  scale_y_log10() +
  xlab('')
```


The second measure of performance is simply the out-of-sample MSE scaled by the
observed variance and then averaged across the 7 series:
\begin{equation}
  \mbox{Average Scaled MSE (p)} = \frac{1}{7} \sum_{i=1}^7
  \frac{\sum_{t=201}^{251} (\hat{x}_{it}^{(p)}-x_{it})^2}
    {\mbox{Var}(x_i)}.
\end{equation}
\autoref{fig:mean-scaled-mse} displays the average of this measure across all `r ncol(y)`
series. Again, about `r round(imp,0)`\% of permutations achieved better average scaled MSE
than the baseline model. The best permutation achieved an average scaled MSE of
`r round(min(df$value), 2)` relative to `r round(sw_sc_test_err, 2)` for the baseline model.
\autoref{fig:series-scaled-mse} shows
boxplots for the scaled MSE separately for each series. Orange dots indicate the
performance of the baseline model.

```{r likelihood-preprocessing}
fpath <- here::here("update2022", "likelihoods.rds")
if (file.exists(fpath)) {
  likelihoods <- read_rds(fpath)
} else {
  likelihoods <- vector("list", nrow(perms))
  for (ii in 1:nrow(perms)) {
    if (ii %% 50 == 0) print(ii)
    ppp = repar(perm_res$par_ests[ii,], prior, TRUE)$outparv
    lpost = getlogLike(ppp, y[perms[ii,], trainset], prior)
    llike = getlogLike(ppp, y[perms[ii,], trainset], prior, ML = TRUE)
    lpostAll = getlogLike(ppp, y[perms[ii,], ], prior)
    llikeAll = getlogLike(ppp, y[perms[ii,], ], prior, ML = TRUE)
    likelihoods[[ii]] <- data.frame(
      llike = llike, llikeAll = llikeAll, lpost = lpost, lpostAll = lpostAll
    )
  }
  likelihoods <- purrr::list_rbind(likelihoods)
  write_rds(likelihoods, fpath)
}
```

```{r likelihood-calculations}
likelihoods <- as_tibble(likelihoods) %>%
  mutate(
    permutation = 1:nperms,
    llikeTest = llikeAll - llike,
    lpostTest = lpostAll - lpost
  )
```

```{r loglike-train, fig.cap="Negative penalized log likelihood (negative log posterior) for each model. The horizontal orange line represents baseline performance. Note that this is an in-sample performance measure.", eval=FALSE}
imp = round(mean(likelihoods$lpost < likelihoods$lpost[truePerm]) * 100, 1)
likelihoods %>% 
  select(lpost) %>% 
  arrange(lpost) %>% 
  mutate(x = 1:nperms) %>%
  ggplot(aes(x, lpost)) + 
  geom_point(color = blue) + 
  geom_hline(yintercept = likelihoods$lpost[truePerm], color = orange) + 
  ylab('penalized loglikelihood (training data)') + 
  xlab(paste0(imp,"% of permutations improved"))
```

```{r loglike-pred-ml,fig.cap="Negative log predictive likelihood for each model. The horizontal red line represents baseline performance. Note that this is an out-of-sample performance measure.", eval=TRUE}
imp = round(mean(likelihoods$llikeTest < likelihoods$llikeTest[truePerm]) * 100, 2)
likelihoods %>% 
  select(llikeTest) %>% 
  arrange(llikeTest) %>% 
  mutate(x = 1:nperms) %>%
  ggplot(aes(x, llikeTest)) + 
  geom_point(color = blue) + 
  geom_hline(yintercept = likelihoods$llikeTest[truePerm], color = orange) + 
  ylab('predictive negative log-likelihood (test data)') + 
  xlab(paste0(imp,"% of permutations improved"))
```

Finally, \autoref{fig:loglike-train} shows the negative log posterior for each
permutation. The orange horizontal line indicates the negative log posterior for
the baseline model.
A total of `r sum(likelihoods$lpost < likelihoods$lpost[truePerm])` permutations resulted in a lower negative
penalized log likelihood. A Bayesian interpretation of these results would be
that none of the permuted "models" are preferable to the true, unpermuted,
economic model.  It should be noted that this is an in-sample measure of
performance and that the Bayesian interpretation is conditional on the
model being true and the priors accurately reflecting expert information.
An out-of-sample evaluation (without penalty) shows that `r imp`\% of the premuted models actually have better prediction performance when evaluated through the likelihood.
\autoref{fig:loglike-pred-ml} gives a visual depiction.

```{r top-20-table-processing}
topn = 20
top20perc <- logperc %>%
  group_by(permutation) %>%
  summarise(value = mean(value)) %>%
  arrange(value) %>% 
  slice_head(n = topn) %>%
  pull(permutation)
top20sc <- sc_test_err %>%
  group_by(permutation) %>%
  summarise(value = mean(value)) %>%
  arrange(value) %>% 
  slice_head(n = topn) %>%
  pull(permutation)
top20llike <- likelihoods %>%
  arrange(llikeTest) %>%
  slice_head(n = topn) %>%
  pull(permutation)
mat = rbind(
  matrix(series_names[perms[top20perc, ]], nrow = topn),
  matrix(series_names[perms[top20sc, ]], nrow = topn),
  matrix(series_names[perms[top20llike, ]], nrow = topn)
)
ndiff = rowSums(mat != matrix(
  series_names[perms[truePerm,]], nrow = 3 * topn, ncol = 7, byrow = T
))
mat = cbind(mat, ndiff)
colnames(mat) = c(series_names,"# different")
mat = as_tibble(mat)
# Use pander?
mat$metric = c(
  rep("series % improvement", topn), 
  rep("scaled MSE", topn), 
  rep("predictive likelihood", topn))
```

```{r best-results}
best_perms <- tibble(
  metric = c(
    "SW model", "Model w/ highest % improvement", 
    "Model w/ lowest scaled MSE", "Model w/ lowest out-of-sample likelihood"),
  permutation = c(truePerm, top20perc[1], top20sc[1], top20llike[1])
)
best_tab = testerr %>% right_join(best_perms, by = "permutation") %>%
  select(-permutation) %>%
  pivot_wider(names_from = name, values_from = value)
kable(best_tab, booktabs = T, digits = 2,col.names = c("", names(best_tab)[-1]),
      caption = "Series MSEs for SW and top models.") %>%
  kable_styling(latex_options = "scale_down")
```

```{r top-20-perc}
mat %>% filter(metric == "series % improvement") %>% 
  select(-metric) %>%
  kable(booktabs = TRUE, caption = "Permutations with highest \\% improvement") %>% 
  kable_styling(latex_options = "scale_down")
```

```{r top-20-scmse}
mat %>% filter(metric == "scaled MSE") %>%
  select(-metric) %>%
  kable(booktabs = TRUE, caption = "Permutations with lowest average scaled MSE") %>% 
  kable_styling(latex_options = "scale_down")
```

```{r top-20-llike}
mat %>% filter(metric == "predictive likelihood") %>%
  select(-metric) %>%
  kable(booktabs = TRUE, caption = "Permutations with lowest negative log likelihood") %>%
  kable_styling(latex_options = "scale_down")
```

\autoref{tab:best-results} shows the error measures separately for each series
of best permutations (as measured by out-of-sample performance) relative to
that of the model fit to the true data. Note that the best permutation is
different for the two measures. Tables \ref{tab:top-20-perc}--\ref{tab:top-20-llike} 
show the `r topn` best
permutations for each evaluation method. There are few commonalities across the
best models. One aspect to note is that output shows up in many
places _except_ where it is supposed to go. Many models perform better with
wages in that position. It is unclear whether this is because
the model is exceptionally bad at predicting output, because that slot likes to
predict series with small variance, or because that slot is simply over-regularized. Note that hours worked, and to lesser extent, the interest rate, are the only series that appear consistently  in the correct places.


```{r parameter-processing}
params <- as.data.frame(perm_res$par_ests)
names(params) = parmGreek
params <- as_tibble(params) %>% 
  mutate(permutation = 1:nrow(perms)) %>%
  pivot_longer(-permutation)
plower = as_tibble(t(prior$lb), .name_repair = "minimal")
prange = as_tibble(t(prior$ub - prior$lb), .name_repair = "minimal")
names(plower) = parmGreek
names(prange) = parmGreek
plower <- pivot_longer(plower, everything(), values_to = "lower")
prange <- pivot_longer(prange, everything(), values_to = "range")
params <- left_join(
  params,
  full_join(plower, prange, by = "name"),
  by = "name"
) %>%
  mutate(scaled = (value - lower) / range)

ptrue = params %>% filter(permutation == truePerm)
```

```{r scaled-parameter-boxplots, fig.cap="Parameter estimates rescaled to [0,1] based on the prior limits. Orange points indicate the SW estimates. Black points are outliers relative to the bulk of the permuted estimates.",fig.height=6}
params %>%
  ggplot(aes(name, scaled)) + 
  geom_boxplot(outlier.shape = ".", width = .5, fill = "cornflowerblue") + 
  coord_flip() + 
  ylab('scaled parameter estimates') + 
  xlab('') + 
  geom_point(data = ptrue, color = orange) +
  scale_x_discrete(labels = label_parse()) +
  scale_y_continuous(expand = expansion())
```

```{r perc-parameter-deviation, fig.cap="Percentage change in estimated parameter relative to the SW estimates. Some large outliers have been removed to better show the bulk.", fig.height=6}
params %>% 
  left_join(ptrue %>% select(name, base = value), by = "name") %>%
  mutate(value = (value - base) / base) %>%
  ggplot(aes(name, value)) + 
  geom_boxplot(outlier.shape = ".", fill = "cornflowerblue") + 
  coord_flip(ylim = c(-5, 25)) + 
  ylab('% change relative to SW model') + 
  xlab('') + 
  geom_hline(yintercept = 0, color = orange) +
  scale_x_discrete(labels = label_parse()) +
  scale_y_continuous(expand = expansion())
```


\autoref{fig:scaled-parameter-boxplots} shows boxplots for the parameter estimates across
permutations scaled to $[0,1]$ by the prior range. Orange dots indicate the SW model
estimates. Clearly, some parameters change very little
from permutation to permutation while others change dramatically. \autoref{fig:perc-parameter-deviation} displays
the same parameter estimates as percent deviations from the SW model estimates.

## Out-of-sample forecasts

```{r perc-improve-preds, echo=FALSE}
dt = seq(to = 2018.75, by = .25, length = ncol(y))
dt = yq(paste(dt %/% 1, dt %% 1 * 4 + 1, sep = '-'))

errs = list()
preds = list()
predPerms = c(truePerm, top20perc)
predList = list() 
for (i in 1:length(predPerms)) {
  ypermed = y[perms[predPerms[i],],]
  filt = getFilterOutput(ypermed, perm_res$par_ests[predPerms[i], ])
  errs[[i]] = filt$vt[unPerm[[predPerms[i]]], ]
  predList[[i]] = as.data.frame(t(y - errs[[i]]))
  preds[[i]] = predList[[i]] %>% 
    pivot_longer(everything(), values_to = as.character(predPerms[i])) %>% 
    select(-name)
}

ydf = as_tibble(t(y))
names(ydf) = series_names
ydf$date = dt
ydf = ydf %>% pivot_longer(-date, names_to = "series")
allpreds = bind_cols(ydf, preds)
names(allpreds)[3:4] = c('observed', 'SW_model')
allpreds <- allpreds %>% filter(date > yq("2006-1"))
truth <- allpreds %>% 
  select(date:SW_model) %>% 
  pivot_longer(observed:SW_model)
flips = allpreds %>% 
  select(-observed, -SW_model) %>%
  pivot_longer(-c(date, series))
```

```{r pc-preds-p1, fig.height=8, fig.cap="Out-of-sample predictions for the best predicting permutations (black), the SW model (orange), and the observed data (blue)."}
flips %>% filter(series %in% series_names[1:4]) %>%
  ggplot(aes(x = date, y = value, group = name)) +
  geom_line(color = 'black', linewidth = .1) + 
  facet_wrap(~ series, scales = 'free_y', ncol = 1, strip.position = 'left') +
  geom_line(data = truth %>% filter(series %in% series_names[1:4]), 
            aes(color = name)) + 
  geom_point(data = truth %>% filter(series %in% series_names[1:4]), 
             aes(color = name), size = 1.5) +
  scale_color_manual(values = c(blue, orange)) + 
  theme(legend.position = 'bottom', 
        legend.title = element_blank(), 
        strip.placement = 'outside',
        strip.background = element_blank()) +
  xlab('') + ylab('')
```

```{r pc-preds-p2, fig.height=6, fig.cap="Out-of-sample predictions for the best predicting permutations (black), the SW model (orange), and the observed data (blue)."}
flips %>% filter(series %in% series_names[5:7]) %>%
  ggplot(aes(x = date, y = value, group = name)) +
  geom_line(color = 'black', linewidth = .1) + 
  facet_wrap(~ series, scales = 'free_y', ncol = 1, strip.position = 'left') +
  geom_line(data = truth %>% filter(series %in% series_names[5:7]), 
            aes(color = name)) + 
  geom_point(data = truth %>% filter(series %in% series_names[5:7]), 
             aes(color = name), size = 1) +
  scale_color_manual(values = c(blue, orange)) + 
  theme(legend.position = 'bottom', 
        legend.title = element_blank(), 
        strip.placement = 'outside',
        strip.background = element_blank()) +
  xlab('') + ylab('')
```




We now examine how well the SW model predicts future data relative to the best models we could have used, had we seen the data. 
\autoref{fig:pc-preds-p1} and \autoref{fig:pc-preds-p2} show the out-of-sample predictions for top `r topn` flips based on "average percent improvement". This analysis is a post-hoc measure as the best models were selected to make these predictions well, though the parameters were estimated without access to this data. We also show the observed data and the predictions from the SW model. The SW model is quite bad at predicting consumption, investment, 
output, and wages. It predicts a consistent 0.5% increase in the real wage level that never appears. In fact, wages are far more volatile than any of the permutations can account for. For the case of investment, output, and consumption, the SW model drastically lags the 2009 recession and underestimates its severity. Furthermore, it continues to predict a much stronger recovery, even 10 years later, than has ever materialized. The SW model is quite accurate for the interest rate, though this should perhaps be expected given that the Taylor rule may well _drive_ Federal Reserve decisions rather than _describe_ them.

<!--

## Changing the truth


```{r simulate-test-sw, eval=FALSE}
bestLlikePvec = par_ests[top20llike[1],]
newPaths = replicate(100, generate(200, bestLlikePvec, nburn=100))
getMSEs <- function(paths, parm, drop = 1:100){
  mses = double(dim(paths)[3])
  preds = array(NA, dim(paths))
  llike = double(dim(paths)[3])
  simsout = gensolution(parm)
  for(i in 1:dim(paths)[3]){
    filt = ss.model(paths[,,i], simsout, output='all')
    preds[,,i] = paths[,,i] - filt$vt
    mses[i] = mean(filt$vt[,-drop]^2)
    llike[i] = -1 * filt$logLik
  }
  return(list(llike = llike, mses=mses, preds=preds))
}
bestPreds = getMSEs(newPaths, bestLlikePvec)
SWPreds = getMSEs(newPaths, pvec$value)
```


What if the permuted model with the highest penalized likelihood were The Truth?  How badly would the SW model fit and forecast?  Here I actually guess it would do OK. 
Procedure: (1) generate data out of the best permutation, (2) get forecasts using it and the SW model (say 100 each). Which is better?
The average negative log-likelihood of the true model is `r #signif(mean(bestPreds$llike),3)` compared to `r #signif(mean(SWPreds$llike),3)` for the SW model. The MSEs are `r #signif(mean(bestPreds$mses),3)` compared to `r #signif(mean(SWPreds$mses),3)` respectively. 

\attn{We actually have that the SW model (unpermuted) has the lowest in-sample negitive log posterior. These were generated using the best unpenalized predicting permutation by log likelihood.}

The next set of figures shows the data (black), predictions from the true model (blue), and SW model (red).


```{r sim-pred-sw, echo=FALSE, eval=FALSE}
drop=1:100
for(j in 1:dim(newPaths)[1]){
  ylims = range(newPaths[j,-drop,], bestPreds$preds[j,-drop,], SWPreds$preds[j,-drop,])
  par(mar=c(5,3,1,1))
  matplot(newPaths[j,-drop,], ty='l', col=1, lty=1, lwd=.1, las=1, bty='n',
       main='', ylab='', xlab=rownames(y)[j], ylim=ylims)
  matlines(bestPreds$preds[j,-drop,], col=4, lty=1, lwd=.1)
  matlines(SWPreds$preds[j,-drop,], col=2, lty=1, lwd=.1)
}
```

-->

\clearpage

# Discussion

As we said in the introduction, there are very few who will defend the
forecasting record of DSGEs.  Rather, their virtues are supposed to lie in
their capturing the structure of the economy, and so providing theoretical
insight, with meaningful parameters, and an ability to evaluate policy and
counterfactuals.  We have examined these claims on behalf of DSGEs through
checking how well a DSGE can be estimated from its own simulation output and by
series permutation.  In both cases, the results are rather negative.

If we take our estimated model and simulate several centuries of data from it,
all in the stationary regime, and then re-estimate the model from the
simulation, the results are disturbing.  Forecasting error remains dismal
and shrinks very slowly with the size of the data.  Much the same is true of
parameter estimates, with the important exception that many of the parameter
estimates seem to be stuck around values which differ from the ones used to
generate the data.  These ill-behaved parameters include not just shock
variances and autocorrelations, but also the "deep" ones whose presence is
supposed to distinguish a micro-founded DSGE from mere time-series analysis or
reduced-form regressions.  All this happens in simulations where the model
specification is correct, where the parameters are constant, and where the
estimation can make use of centuries of stationary data, far more than will ever be
available for the actual macroeconomy.

If we randomly re-label the macroeconomic time series and feed them into the
DSGE, the results are no more comforting.  Much of the time we get a model
which predicts the (permuted) data _better_ than the model predicts the
unpermuted data.  Even if one disdains forecasting as end in itself, it is hard
to see how this is at all compatible with a model capturing something ---
anything --- essential about the structure of the economy.[^align]  Perhaps
even more disturbing, many of the parameters of the model are essentially
unchanged under permutation, including "deep" parameters supposedly
representing tastes, technologies and institutions.

[^align]: It's conceivable that the issue is one of measurement.  The variables in the DSGE model are defined by their roles in the inter-temporal optimization problem.  (Thus labor makes a negative contribution to present utility but a positive contribution to present output, cannot be stored from period to period, etc.)  The series gathered by the official statistical agencies use quite distinct definitions, and nothing guarantees that the series with analogous names are good measurements of the theoretical variables.  (Cf. \citet[p. 6]{Haavelmo-probability-approach}: "there is hardly an economist who feels really happy about identifying current series of 'national income,' 'consumption,', etc., with the variables by these names in his theories".)  Thus, perhaps, the model is right, but GDPDEF is really a better proxy for \emph{labor}, $l_t$, than PRS85006023 is.  This would, needless to say, raise its own set of difficulties for the interpretation and use of these models.  One possible route forward would be to use _many_ observables which are all imperfectly aligned with the theoretical variables, which would, perhaps, require imposing a factor-model structure on the relations between observables and state variables.  \citet{Boivin-Giannoni-DSGEs-in-data-rich-environment} is a step in this direction.

To take this analysis one step further, we can examine the "best" predicting 
model, in terms of out-of-sample predictive log-likelihood. Recall, this is just one 
of about `r round(mean(likelihoods$llikeTest < likelihoods$llikeTest[truePerm]), 2)`\% which forecast better. It is also important to note here, that the penalized negative log-likelihood 
(out-of-sample) is very flat relative to permutations. In terms of that metric, 
`r signif(mean(likelihoods$llikeTest / likelihoods$llikeTest[truePerm] < 1.1), 2) * 100`% of permutations 
are within 10% of the true permutation.
The true model arranges the data as
\begin{quotation}
`r series_names[perms[truePerm, ]]`
\end{quotation}
\noindent while the best one by out-of-sample log-likelihood is 
\begin{quotation}
`r series_names[perms[top20llike[1],]]`.
\end{quotation}
\noindent This model is pretty clearly scary to a macroeconomist. 
In the Appendix, we take this model to 
most implausible extreme.
We rewrite the introduction of \citet{SmetsWouters2007}, permuting all the series 
to reflect the best one by negative predictive log likelihood. Such a description
of the macroeconomy is doubtless complete nonsense, and yet this interpretation 
would have led to better predictions of macroeconomic comovements.


Ignoring the fact that the DSGE provides generally poor forecasts relative to 
alternative, yet uninterpretable, similar models, one might wonder whether it manages
to avoid the Lucas Critique. That is, can policy makers imagine that the model truly decouples policy variables from the "deep" parameters that economic actors maintain?
Specifically, for example, if the Fed changes how they manage interest rates,
do the other parameters move? This question can be investigated directly by
examining the distribution of Taylor Rule parameters conditional on the
truth generating the model and estimating said parameters in the simulation exercise.[^taylor]  These correlations are shown in \autoref{tab:covary-with-Taylor}. The Taylor rule parameters are, $\rho$, the autocorrelation in the interest rate,
$r_\pi$, the response to inflation, $r_y$, the response to deviations of output
from potential, and $r_{\Delta y}$ the response to changes in the deviation.  A
handful of deep parameters have correlations larger in magnitude than 0.3
(shown in bold). This magnitude may or may not be large enough for concern, but
the correlation is also not negligible. So the question of whether these models
actually address their original motivation is, at the very least, still open.

[^taylor]: This analysis is not constrained to 
the distribution of plausible Taylor rule parameters that the Fed might
consider, but rather simply investigates all possible such parameters.

```{r covary-with-Taylor, results="asis"}
# The 'deep' parameters begin with 'phi1'
# The Taylor rule parameters are 'rho' (changes in the interest rate should be smooth)
#             'r[pi]' (response to inflation)
#             'r[y]' (response to deviation of output from potential)
#             'r[Delta][y]' (response to changes in output deviation)
parmDeep = parmGreek[which(parmGreek == 'phi1'):length(parmGreek)]
parmTaylor = parmGreek[seq(from = which(parmGreek == 'r[pi]'), length.out = 4)]
pars <- parm_error %>% 
  select(parm, value) %>% 
  filter(parm %in% unique(c(parmDeep, parmTaylor))) %>%
  pivot_wider(names_from = parm, values_from = value) %>% 
  unnest(everything()) %>%
  as.matrix()
covary = as_tibble(cor(pars)[parmDeep, parmTaylor])
covary$parm = parameter_latex[which(parmGreek == 'phi1'):length(parmGreek)]
covary = covary %>% relocate(parm)


covary %>% 
  mutate(
    across(
      where(is.numeric), 
      ~ cell_spec(round(.x, 2), bold = text_spec(abs(.x) > .3)
      ))) %>%
  knitr::kable(
    digits = 2, 
    booktabs = TRUE, 
    align = "r",
    caption = "Correlations between ``deep'' parameters and Taylor rule parameters from the ``Simulate and estimate'' exercise.", 
    escape = FALSE, 
    col.names = c("", parameter_latex[seq(from = which(parmGreek == 'r[pi]'), length.out = 4)]))
```


The results of the two tests described here are grim.  Series swapping gives us strong reasons
to doubt that the DSGE machinery manages to capture anything important about
the structure of the economy.  Even if one dismisses that, and believes
(perhaps because "theory is evidence too") that the DSGE must be right, the
simulation exercise shows that, even under the most favorable possible
circumstances, it is simply wrong to think that the DSGE will give reasonably
accurate predictions, or even that it can be reliably estimated.

We have not, of course, proved that flaws like this are inherent in the DSGE
form.  But we have _not_ cherry-picked an obsolete or marginal
model.\footnote{We obtained very similar results for the real-business-cycle
model of \citet{KydlandPrescott1982}, but omit them here, because that model
\emph{is} obsolete.}  The SW model is widely regarded as the baseline DSGE for the
economy of the United States, which is by far the most important national
economy in the world.  The SW paper has been cited over 6300
times.\footnote{Google Scholar
(\url{https://scholar.google.com/scholar?cites=8854430771281116653}), accessed
27 October 2022.} What concerns us is that in all that literature, we appear to
be the first to have subjected it to such direct, even elementary, tests.  We
do not assert that all DSGE models must be pathological in the ways we have
shown the SW model is.  Indeed, readers are free to hope that their favorite
DSGE does capture economic structure and can be meaningfully estimated with
reasonable amounts of data.  But we hope we have persuaded readers that they
can, and should, do more than hope: they can _check_.




\clearpage

\appendix

# Data preprocessing {#sec:data-preprocessing}

\footnotesize

The
necessary series are shown in \autoref{tab:data}. All of the data are
quarterly. The required series are GDPC1, GDPDEF, PCEC, FPI, CE16OV, FEDFUNDS,
CNP16OV, PRS85006023, and COMPNFB.  These nine series are used to create $x_t$
as follows:
\begin{align*}
  pop_t^{ind} &= CNP16OV_t / CNP16OV_{2012Q3},\\
  emp_t^{ind} &= CE16OV_t / CE16OV_{2012Q3},\\
  y_t &= 100\ln\left(\frac{GDPC1_t}{pop_t^{ind}}\right),\\
  c_t &= 100\ln\left(\frac{PCEC_t/GDPDEF_t}{pop_t^{ind}}\right),\\
  i_t &= 100\ln\left(\frac{FPI_t/GDPDEF_t}{pop_t^{ind}}\right),\\
  l_t &=
        100\ln\left(\frac{PRS85006023_t/emp_t^{ind}}{pop_t^{ind}}\right),\\
  \pi_t &= 100\ln\left(\frac{GDPDEF_t}{GDPDEF_{t-1}}\right),\\
  w_t &= 100\ln\left(\frac{COMPNFB_t}{GDPDEF_t}\right),\\
  r_t &= FEDFUNDS/4.
\end{align*}
\begin{table}[h]
  \centering
  \resizebox{\linewidth}{!}{
  \begin{tabular}{@{}lll@{}}
    \toprule
    Series ID & Description & Unit \\
    \midrule
    GDPC1 & Real Gross Domestic Product & Billions of Chained 2012 \$\\
    GDPDEF & GDP Implicit Price Deflator & Index: 2012=100\\
    PCEC & Personal Consumption Expenditures & Billions of \$\\
    FPI & Fixed Private Investment & Billions of \$\\
    CE16OV & Civilian Employment & Thousands of persons\\
    FEDFUNDS & Effective Federal Funds Rate & Percent\\
    CNP16OV & Civilian Noninstitutional Population & Thousands of
                                                     persons\\
    PRS85006023 & Nonfarm business sector: average weekly hours &
                                                                  Index:
                                                                  2012=100\\
    COMPNFB & Nonfarm business sector: Compensation per hour & Index: 2012=100\\
    \bottomrule
  \end{tabular}
  }
  \caption{Data series from FRED for estimating the DSGE.}
  \label{tab:data}
\end{table}


\clearpage

# Rewriting \citet{SmetsWouters2007}

In the rest of this section, we describe the log-linearized version of the DSGE
model that we subsequently estimate using US data. All variables are
log-linearized around their steady-state balanced growth path. Starred
variables denote steady-state values. We first describe the aggregate demand
side of the model and then turn to the aggregate supply.

The aggregate resource constraint is given by
\begin{equation}
\label{eq:1}
\yobs{t} =   c_y\cobs{t} +  i_y \iobs{t} + z_y z_t +
\epsilon^g_t.
\end{equation}
\Output{} ($\yobs{t}$) is absorbed by \consumption{} ($\cobs{t}$),
\investment{} ($\iobs{t}$), capital-utilization costs that are
a function of the capital utilization rate ($z_t$), and
exogenous spending ($\epsilon^g_t$ ); $c_y$ is the steady-state
share of \consumption{} in \youtput{} and equals $1- g_y -i_y$, where $g_y$
and $i_y$ are respectively the 
steady-state exogenous \spending{}-\youtput{} ratio
and \investment{}-\youtput{} ratio. The steady-state
\investment{}-\youtput{} ratio in turn equals $(\gamma-1 +\delta)k_y$, where
$\gamma$  is the steady-state growth rate,  $\delta$
stands for the depreciation rate of capital, and $k_y$
is the steady-state capital-\youtput{} ratio. Finally,
$z_y=  R_*^k ky$, where $R_*^k$ is the steady-state rental
rate of capital. We assume that exogenous
\spending{} follows a first-order autoregressive
process with an IID-Normal error term and is
also affected by the productivity shock as fol-
lows: $\epsilon^g_t = \rho_g\epsilon^g_{t-1} + \eta_t^g
+\rho_{ga}\eta_t^a$. The latter is 
empirically motivated by the fact that, in estimation, exogenous
\spending also includes net exports, which may be affected by domestic
productivity developments. 

The dynamics of \consumption{} follows from the \consumption{} Euler
equation and is given by 
\begin{equation}
  \cobs{t} = c_1\cobs{t-1} + (1-c_1)\E_t[\cobs{t+1}] + c_2 (\lobs{t}
  -\E_t[\lobs{t+1}]) - c_3(\robs{t}-\E_t[\piobs{t+1}])+\epsilon_t^b),
\end{equation}
where $c_1 = (\lambda/\gamma)/(1+\lambda/\gamma)$, $c_2 = [(\sigma_c -
1)(W_*^h L_*/C_*)] / [\sigma_c(1+\lambda/\gamma)],$ and $c_3 =
(1-\lambda/\gamma)/[\sigma_c(1+\lambda/\gamma)]$. Current \consumption{}
$(\cobs{t}$ depends on a weighted average of past and expected future
\consumption, and on expected growth in \hours{}
$\lobs{t}
  -\E_t[\lobs{t+1}])$, the ex ante \realinterest{}
$(\robs{t}-\E_t[\piobs{t+1}]$, and a disturbance term $\epsilon^b_t$ . Under
the assumption of no external habit formation
$(\lambda=0)$ and log utility in \consumption{} $(\sigma_c =1)$,
$c1 = c2 = 0$ and the traditional purely forward-looking
\consumption{} equation is obtained. With 
steady-state growth, the growth rate $\gamma$ marginally
affects the reduced-form parameters in the linearized \consumption{}
equation. When the elasticity of 
intertemporal substitution (for constant \labor) is
smaller than one $(\sigma_c > 1)$, \consumption{} and \hours{} are
complements in utility and  \consumption{} depends positively on
current \hours{} and negatively on expected growth in \hours{} (see
Susanto Basu and Kimball 2002). Finally,the disturbance term $\epsilon_t^b$
represents a wedge between the \interest{} controlled by the
central bank and the return on assets held by the households. A
positive shock to this wedge increases the required return on assets
and reduces current \consumption. At the same time, it also increases
the cost of capital and reduces the value of capital and \investment,
as shown below.

The dynamics of \investment{} comes from the \investment{} Euler
equation and is given by
\begin{equation}
  \iobs{t} = i_1\iobs{t-1} + (1-i_1)\E[\iobs{t+1}] + i_2 q_t + \epsilon_t^i,
\end{equation}
where $i_1 = 1/(1+\beta\gamma^{1-\sigma_c}),$ $i_2 =
1/[(1+\beta\gamma^{1-\sigma_c})\gamma^2\varphi]$, $\varphi$ is the
steady-state elasticity of the capital adjustment cost function, and
is the discount factor applied by households. As in \citet{ChristianoEichenbaum2005}, a
higher elasticity of the cost of adjusting capital reduces the
sensitivity of \investment{} ($\iobs{t}$) to the real value of the
existing capital stock ($q_t$). Modeling capital adjustment costs as a
function of the change in \investment{} rather than its level
introduces additional dynamics in the \investment{} equation, which is 
useful in capturing the hump-shaped response of \investment{} to various
shocks. Finally, $\epsilon^i_t$ represents a disturbance to the
\investment-specific technology process and is assumed to follow a 
first-order autoregressive process with an IID-Normal error term:
$\epsilon_t^i = \rho_i\epsilon_{t-1}^i + \eta_t^i$.

The corresponding arbitrage equation for the
value of capital is given by
\begin{equation}
q_t = q_1\E_t[q_{t+1}] + (1-q_1)\E_t[r_{t+1}^k] - (\robs{t} -
\E[\piobs{t+1}] + \epsilon_t^b),
\end{equation}
where $q_1 = \beta\gamma^{-\sigma_c}(1-\delta) = [(1-\delta)/(R_*^k +
(1-\delta))]$. The current value of the capital stock ($q_t$) depends
positively on its expected future value and the expected real rental
rate on capital ($\E_t[r_{t+1}^k]$) and negatively on the ex ante
\realinterest{} and the risk premium disturbance.

Turning to the supply side, the aggregate production function is given by
\begin{equation}
  \yobs{t} = \phi_p(\alpha k_t^2 + (1-\alpha)\lobs{t} + \epsilon_t^a).
\end{equation}
\Output{} is produced using capital ($k_t^s$) and \labor{} services
(\hours, $\lobs{t}$). Total factor productivity ($\epsilon_t^a$) is
assumed to follow a first-order autoregressive process: $\epsilon_t^a
= \rho_a \epsilon_{t-1}^a + \eta_t^a$. The parameter $\alpha$ captures
the share of capital in production, and the parameter $\phi_p$ is one
plus the share of fixed costs in production, reflection the presence
of fixed costs in production.

As newly installed capital becomes effective only with a one-quarter
lag, current capital services used in production $(k_t^s)$ are a
function of capital installed in the previous period $(k_{t-1})$ and
the degree of capital utilization $(z_t)$:
\begin{equation}
  k_t^s = k_{t-1} + z_t.
\end{equation}
Cost minimization by the households that provide capital services
implies that the degree of capital utilization is a positive function
of the rental rate of capital,
\begin{equation}
  \label{eq:7}
  z_t = z_1 r_t^k,
\end{equation}
where $z_q = (1-\psi)/\psi$ and $\psi$ is a positive function of the
elasticity of the capital utilization adjustment cost function and
normalized to be between zero and one. When $\psi=1$, it is extremely
costly to change the utilization of capital and, as a result, the
utilization of capital remains constant. In contrast, when $\psi=0$,
the marginal cost of changing the utilization of capital is constant
and, as a result, in equilibrium the rental rate on capital is
constant, as is clear from equation \eqref{eq:7}.

The accumulation of installed capital $(k_t)$ is a function not only
of the flow of \investment{} but also of the relative efficiency of
these \investment{} expenditures as captured by the
\investment-specific technology disturbance
\begin{equation}
  \label{eq:8}
  k_t = k_1 k_{t-1} + (1-k_1) \iobs{t} + k_2 \epsilon_{t}^i,
\end{equation}
with $k_1=(1-\delta)/\gamma$ and $k_2 = (1-(1-\delta)/\gamma) (1+
\beta\gamma^{1-\sigma_c})\gamma^2\varphi.$


Turning to the monopolistic competitive goods market, cost
minimization by firms implies that the price mark-up ($\mu_t^p$),
defined as the difference between the average price and the nominal
marginal cost or the negative of the real marginal cost, is equal to
the difference between the marginal product of \labor{} ($mp\lobs{t}$)
and the real \wage{} ($\wobs{t}$):
\begin{equation}
  \label{eq:9}
  \mu_t^p = mp\lobs{t} - \wobs{t} = \alpha(k_t^s - \lobs{t}) +
  \epsilon_t^a - \wobs{t}.
\end{equation}
As implied by the second equality in \eqref{eq:9}, the marginal
product of \labor{} is itself a positive function of the
capital-\labor{} ratio and total factor productivity.

Due to \price{} stickiness, as in Calvo (1983), and partial indexation to
lagged \inflation{} of those \prices{} that can not be reoptimized, as in
Smets and Wouters (2003), \prices{} adjust only sluggishly to their
desired mark-up. Profit maximization by \price-setting firms gives
rise to the following New-Keynesian Phillips curve:
\begin{equation}
  \label{eq:10}
  \piobs{t} = \pi_1\piobs{t-1} + \pi_2 \E_t[\piobs{t+1}] - \pi_3
  \mu_t^p + \epsilon_t^p,
\end{equation}
where $\pi_1 = \iota_p / (1+ \beta\gamma^{1-\sigma_c} \iota_p),$
$\pi_2 = \beta \gamma^{1-\sigma_c} / (1+\beta \gamma^{1-\sigma_c}
\iota_p),$ and $\pi_3 = 1/(1 +\beta \gamma^{1-\sigma_c} \iota_p)[(1-
\beta \gamma^{1-\sigma_c} \xi_p)(1-\xi_p) / \xi_p ((\phi_p -
1)\epsilon_p +1)].$ \Inflation{} $(\piobs{t})$ depends positively on
past and expected future \inflation, negatively on the current
\price{} mark-up, and positively on a \price{} mark-up disturbance
$(\epsilon_t^p)$. The \price{} mark-up disturbance is assumed to
follow an ARMA(1,1) process: $\epsilon_t^p = \rho_p \epsilon_{t-1}^p +
\eta_t^p - \mu_p \eta_{t-1}^p$, where $\eta_t^p$ is an IID-Normal
\price mark-up shock. The inclusion of the MA term is designed to
capture the high-frequency fluctuations in \inflation.

When the degree of indexation to past \inflation{} is zero ($\iota_p =0$),
equation \eqref{eq:10} reverts to a standard, purely forward-looking Phillips
curve ($\pi_1 = 0$). The assumption that all \prices{} are indexed to either
lagged \inflation{} or the steady-state \inflationrate{} ensures that the
Phillips curve is vertical in the long run. The speed of adjustment
to the desired mark-up depends, among others, on the degree of \price-stickiness ($\xi_p$), the curvature of the Kimball goods market aggregator ($\epsilon_p$), and the steady-state mark-up, which in equilibrium is
itself related to the share of fixed costs in production ($\phi_p -  1$)
through a zero-profit condition. A higher  $\epsilon_p$ slows down the speed of
adjustment because it increases the strategic complementarity with
other \price{} setters. When all \prices{} are flexible ($\xi_p = 0$) and the
\price-mark-up shock is zero, equation \eqref{eq:10} reduces to the familiar
condition that the \price{} mark-up is constant, or equivalently that
there are no fluctuations in the wedge between the marginal product of
\labor{} and the real \wage. 

Cost minimization by firms will also imply that the rental rate of
capital is negatively related to the capital-\labor{} ratio and
positively to the real \wage{} (both with unitary elasticity): 
\begin{equation}
  \label{eq:11}
  r_t^k = -(k_t - \lobs{t}) + \wobs{t}
\end{equation}
In analogy with the goods market, in the monopolistically
competitive \labor{} market, the \wage{} mark-up will be equal to the
difference between the real \wage{} and the marginal rate of substitution
between \working{} and \consuming{} ($mrs_t$),
\begin{equation}
  \label{eq:12}
  \mu_t^w = \wobs{t} - mrs_t = \wobs{t} - (\sigma_l\lobs{t} +
  \frac{1}{1-\lambda/\gamma} \left(\cobs{t} - \frac{\lambda\cobs{t-1}}{\gamma}\right),
\end{equation}
where $\sigma_l$ is the elasticity of \labor{} supply with respect to
the real \wage{} and $\lambda$ is the habit parameter in \consumption.

Similarly, due to nominal \wage{} stickiness and partial indexation of
\wages{} to \inflation, real \wages{} adjust only gradually to the
desired \wage{} mark-up:
\begin{equation}
  \label{eq:13}
  \wobs{t} = w_1\wobs{t-1} + (1- w_1)(\E_t[\wobs{t+1}] +
  \E_t[\piobs{t+1}]) - w_2 \piobs{t} + w_3 \piobs{t-1} - w_4 \mu_t^w + \epsilon_t^w,
\end{equation}
with $w_1 = 1/(1+\beta\gamma^{1-\sigma_c})$, $w_2=
(1+\beta\gamma^{1-\sigma_c}\iota_w) / (1+\beta\gamma^{1-\sigma_c})$,
$w_3 = \iota_w / (1+\beta\gamma^{1-\sigma_c}),$ and $w_4 = 1/
(1+\beta\gamma^{1-\sigma_c})
[(1-\beta\gamma^{1-\sigma_c}\xi_w)(1-\xi_w) /
(\xi_w((\phi_w-1)\epsilon_w + 1))]$.


The real \wage{} $\wobs{t}$ is a function of expected
and past real \wages, expected, current, and past
\inflation, the \wage{} mark-up, and a \wage-markup disturbance $(\epsilon^w_t)$. If \wages{} are perfectly
flexible $(\xi_w = 0)$, the real \wage{} is a constant
mark-up over the marginal rate of substitution
between \consumption{} and \leisure{}. In general,
the speed of adjustment to the desired \wage{}
mark-up depends on the degree of \wage{} stickiness $(\xi_w)$ and the
demand elasticity for \labor, which itself is a function of the steady-state
\labor{} market mark-up $(\phi_w - 1)$ and the curvature of the
Kimball \labor{} market aggregator 
$(\epsilon_w)$. When \wage{} indexation is zero $(\iota_w =0)$,
real \wages{} do not depend on lagged \inflation{}
$(w_3 = 0)$. The \wage{}-markup disturbance $(\epsilon_w^t)$ is
assumed to follow an ARMA(1, 1) process with an IID-Normal error term:
$\epsilon_t^w = \rho_w \epsilon_{t-1}^w + \eta_t^w - \mu_w
\eta_{t-1}^w$. As is the case of the \price{} mark-up shock, the
inclusion of an MA term allows us to pick up some of the
high-frequency fluctuations in \wages{}.\footnote{Alternatively, we
  could interpret this disturbance as a \labor{} supply disturbance
  coming changes in preferences for \leisure{}.} 

Finally, the model is closed by adding the following empirical
monetary policy reaction function:
\begin{align}
  \label{eq:14}
  \robs{t} &= \rho\robs{t-1} + (1-\rho)\left[r_\pi \piobs{t} +
  r_y(\yobs{t}-\yobs{t}^p)\right]+\\ &\quad + r_{\Delta y}\left[(\yobs{t}-\yobs{t}^{p}) -
  (\yobs{t-1} - \yobs{t-1}^{p})\right] + \epsilon_t^r.\notag
\end{align}

The monetary authorities follow a generalized Taylor rule by gradually
adjusting the policy-controlled \interest{} $(\robs{t})$ in response
to \inflation{} and the \youtput{} gap, defined as the difference between
actual and potential \youtput{} \citep{Taylor-on-Taylor-rule}. Consistently
with the DSGE model, potential \youtput{} is defined as the level of
\youtput{} that would prevail under flexible \prices{} and \wages{} in
the absence of the two ``mark-up'' shocks.\footnote{In practical
  terms, we expand the model consisting of equations \eqref{eq:1} to
  \eqref{eq:14} with a flexible-\price-and-\wage{} version in order to
  calculate the model-consistent \youtput{} gap. Note that the assumption
  of treating the \wage{} equation disturbance as a \wage{} mark-up
  disturbance rather than a \labor{} supply disturbance coming from
  changed preferences has implications for our calculation of
  potential \youtput{}. } 

The parameter $\rho$ captures the degree of
\interest{} smoothing. In addition, there is a short-run feedback from
the change in the \youtput{} gap. Finally, we assume that the monetary
policy shocks $(\epsilon_t^r)$ follow a first-order autoregressive
process with an IID-Normal error term: $\epsilon_t^r =
\rho_r\epsilon_{t-1}^r + \eta_t^r$.

Equations \eqref{eq:1} to \eqref{eq:14} determine 14 endogenous
variables: $y_t,\ c_t,\ i_t,$ $q_t,\ k_t^s,$ $k_t,\ z_t,\ r_t^k,$
$\mu_t^p,\ \pi_t,\ \mu_t^w,\ w_t,\ l_t,$ and $r_t$. The stochastic
behavior of the system of linear rational expectations equations is
driven by seven exogenous disturbances: total factor productivity
$(\epsilon^a_t)$, investment-specific technology $(\epsilon^i_t)$, risk premium $(\epsilon_t^b)$, 
exogenous spending $(\epsilon_t^g)$, \price{} mark-up
$(\epsilon_t^p)$, \wage{} mark-up $(\epsilon_t^w)$, and monetary
policy $(\epsilon_t^r)$ shocks. Next we turn to the estimation of 
the model. 

